{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VV0d2UNSiE-n"
   },
   "source": [
    "# Overview\n",
    "The basic idea of this notebook is to transform the data from a sequence of letters into possible categories using a CNN. We use letters instead of words since we say in the [mothjer](https://www.kaggle.com/fcostartistican/don-t-mess-with-my-mothjer) notebook that words are often misspelled or written differently so looking at character level correlations might work better.  We utilize Atrous Convolutions since they can account for larger spacings between relevant words and ideas. For the model we focus on individual letters and ngrams sized 1-10, but the model could easily be expanded to handle larger differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TJwxuPssiE-s"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Bidirectional, CuDNNGRU, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in /home/stgc/anaconda2/lib/python2.7/site-packages\n",
      "Collecting pyyaml (from keras)\n",
      "  Using cached PyYAML-3.12.tar.gz\n",
      "Collecting six>=1.9.0 (from keras)\n",
      "  Using cached six-1.11.0-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: scipy>=0.14 in /home/stgc/anaconda2/lib/python2.7/site-packages (from keras)\n",
      "Collecting numpy>=1.9.1 (from keras)\n",
      "  Downloading numpy-1.14.2-cp27-cp27mu-manylinux1_x86_64.whl (12.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.1MB 154kB/s ta 0:00:0101\n",
      "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
      "  Running setup.py bdist_wheel for pyyaml ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/stgc/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, six, numpy\n",
      "  Found existing installation: PyYAML 3.11\n",
      "    Uninstalling PyYAML-3.11:\n",
      "      Successfully uninstalled PyYAML-3.11\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: numpy 1.13.3\n",
      "    Uninstalling numpy-1.13.3:\n",
      "      Successfully uninstalled numpy-1.13.3\n",
      "Successfully installed numpy-1.14.2 pyyaml-3.12 six-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-pj-AKOZiE-6"
   },
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "max_features = 64\n",
    "maxlen = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vndQY7DLibf9"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "!apt-get install graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "J0nHMEVAhjcB"
   },
   "outputs": [],
   "source": [
    "!pip install pydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "saEEN-kAyOmI"
   },
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZGUNFN2rxwF_"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "     # 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-z-xOOhByBxH"
   },
   "outputs": [],
   "source": [
    "#if upload from local\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "j_bf4NGBjHfn"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "42a-nkWejGag"
   },
   "outputs": [],
   "source": [
    "\n",
    "files.download('aug_gruconv_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8xtVZgd96bGO"
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_import = drive.CreateFile({'id':'12s1UHj-QmX6ABYDH7bq4CmhQaPEfvqz9'})\n",
    "\n",
    "csv_import.GetContentFile('clean_test_third.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7BpieeQph7Wn"
   },
   "outputs": [],
   "source": [
    "csv_import = drive.CreateFile({'id':'1jzsPgPBk13gXl4caOlDOTX1M3Eo1S-Nt'})\n",
    "csv_import.GetContentFile('clean_train_ori_third.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xPchvndn-D1Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_import = drive.CreateFile({'id':'1eJQ32N7O1uGZV7vYi8zvISCJ3KkdYoQp'})\n",
    "\n",
    "csv_import.GetContentFile('test_translated_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "id_pVPGx-FCS"
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_import = drive.CreateFile({'id':'1q1btNUTkqQ0-Mfr7RiiuNZ7i3sai1a0L'})\n",
    "\n",
    "csv_import.GetContentFile('train_translated_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NpAwJomC6w-y"
   },
   "outputs": [],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaPZ_4DEiE_F"
   },
   "source": [
    "# Load and Preprocessing Steps\n",
    "Here we load the data and fill in the misisng values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ONEjicpgiE_I"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv(\"clean_train_ori_third.csv\")\n",
    "test = pd.read_csv(\"clean_test_third.csv\")\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"unknown\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"unknown\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KoFBMrlm7Q-4"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nYivLcUiE_V"
   },
   "source": [
    "## Sequence Generation\n",
    "Here we take the data and generate sequences from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zJ7_1VU1iE_X"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "akpvNXCwiE_c"
   },
   "outputs": [],
   "source": [
    "def build_model(conv_layers = 2, \n",
    "                dilation_rates = [0, 2, 4, 8, 16], \n",
    "                embed_size = 256):\n",
    "    inp = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer.word_counts)+1, \n",
    "                  output_dim = embed_size)(inp)\n",
    "    prefilt_x = Dropout(0.25)(x)\n",
    "    out_conv = []\n",
    "    # dilation rate lets us use ngrams and skip grams to process \n",
    "    for dilation_rate in dilation_rates:\n",
    "        x = prefilt_x\n",
    "        for i in range(2):\n",
    "            if dilation_rate>0:\n",
    "                x = Conv1D(16*2**(i), \n",
    "                           kernel_size = 3, \n",
    "                           dilation_rate = dilation_rate,\n",
    "                          activation = 'relu',\n",
    "                          name = 'ngram_{}_cnn_{}'.format(dilation_rate, i)\n",
    "                          )(x)\n",
    "            else:\n",
    "                x = Conv1D(16*2**(i), \n",
    "                           kernel_size = 1,\n",
    "                          activation = 'relu',\n",
    "                          name = 'word_fcl_{}'.format(i))(x)\n",
    "        out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    x = concatenate(out_conv, axis = -1)    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4O66nCXiE_j"
   },
   "source": [
    "# Train the Model\n",
    "Here we train the model and use model checkpointing and early stopping to keep only the best version of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_O3wy9PiE_l"
   },
   "source": [
    "## Hold-out\n",
    "We create a hold-out group of data for having a set of data the model was never exposed to for testing it. We add all of the possible categories together as a cheap hack for ensuring groups are somewhat stratified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KlrcUg72iE_s"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "any_category_positive = np.sum(y,1)\n",
    "print('Distribution of Total Positive Labels (important for validation)')\n",
    "print(pd.value_counts(any_category_positive))\n",
    "X_t_train, X_t_test, y_train, y_test = train_test_split(X_t, y, \n",
    "                                                        test_size = 0.2, \n",
    "                                                        stratify = any_category_positive,\n",
    "                                                       random_state = 2017)\n",
    "print('Training:', X_t_train.shape)\n",
    "print('Testing:', X_t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eNiSR-79iE_4"
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # large enough that some other labels come in\n",
    "epochs = 10\n",
    "\n",
    "file_path=\"best_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t_train, y_train, \n",
    "          validation_data=(X_t_test, y_test),\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle = True,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgGQqqa6iFAD"
   },
   "source": [
    "# Make Predictions\n",
    "Load the model and make predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "olp49dryiFAG"
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "y_test = model.predict(X_te)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KZzs0d4CrWht"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XvFZK1jb9nMv"
   },
   "outputs": [],
   "source": [
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xFUERl-2LssA"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6kq3hSbNhKKS"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wi1D7lPXNaJu"
   },
   "outputs": [],
   "source": [
    "!unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kJfiqOzEP14-"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zYwrN94jiFAl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Input, Dense, Embedding, MaxPooling1D, Conv1D, SpatialDropout1D\n",
    "from keras.layers import add, Dropout, PReLU, BatchNormalization, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "from keras import initializers, regularizers, constraints, callbacks\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "\n",
    "\n",
    "#EMBEDDING_FILE = 'glove.twitter.27B.200d.txt'\n",
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "#EMBEDDING_FILE = 'wiki.simple.vec'\n",
    "\n",
    "train = pd.read_csv('train_translated_clean.csv')\n",
    "test = pd.read_csv('test_translated_clean.csv')\n",
    "submission = pd.read_csv('test_translated_sp_clean.csv')\n",
    "\n",
    "X_train = train[\"comment_text_english\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text_english\"].fillna(\"fillna\").values\n",
    "\n",
    "\n",
    "max_features = 160000\n",
    "maxlen = 300\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "\n",
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 32\n",
    "\n",
    "def get_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "    \n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n",
    "    maxpool_3 = MaxPool2D(pool_size=(maxlen - filter_sizes[3] + 1, 1))(conv_3)\n",
    "        \n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "        \n",
    "    outp = Dense(6, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fMxQhWBMYdez"
   },
   "outputs": [],
   "source": [
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, CuDNNLSTM\n",
    "gru_len = 100\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.4\n",
    "rate_drop_dense = 0.35\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "\n",
    "\n",
    "def get_cap_model():\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(max_features,\n",
    "                            embed_size,\n",
    "                            input_length=maxlen,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "    \n",
    "    \n",
    "    bi = Bidirectional(\n",
    "        CuDNNLSTM(gru_len, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    #bi = Dropout(dropout_p)(bi)\n",
    "    #gm = GlobalMaxPool1D()(bi)\n",
    "    #x = Reshape((maxlen, embed_size, 1))(embed_layer)\n",
    "    \n",
    "    #conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n",
    "    #                                                                                activation='elu')(x)\n",
    "    #primarycaps = PrimaryCap(x, dim_capsule=4, n_channels=16, kernel_size=8, strides=3, padding='valid')\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(bi)\n",
    "    #att = Attention()(bi)\n",
    "    #att = Reshape((256, 1))(att)\n",
    "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
    "    #att = Flatten()(att)\n",
    "    #att = Dense(16)(att)\n",
    "    #att = Reshape((1, 16))(att)\n",
    "    #capsule = Dropout(dropout_p)(capsule)\n",
    "    #capsule = Reshape((1, 202))(capsule)\n",
    "    capsule = Flatten()(capsule)\n",
    "    #conc = concatenate([capsule, gm, att])\n",
    "    #capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(6, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    adam = optimizers.Adam(lr=0.001)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=adam ,\n",
    "        metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_JP463yYnLox"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uBdC0sU51AuX"
   },
   "outputs": [],
   "source": [
    "def dpcnn():\n",
    "    filter_nr = 64\n",
    "    filter_size = 3\n",
    "    max_pool_size = 3\n",
    "    max_pool_strides = 2\n",
    "    dense_nr = 256\n",
    "    spatial_dropout = 0.2\n",
    "    dense_dropout = 0.5\n",
    "    train_embed = False\n",
    "    comment = Input(shape=(maxlen,))\n",
    "    emb_comment = Embedding(max_features, embed_size, weights=[embedding_matrix])(comment)\n",
    "    emb_comment = SpatialDropout1D(spatial_dropout)(emb_comment)\n",
    "    emb_comment = Bidirectional(CuDNNGRU(64, return_sequences=True))(emb_comment)\n",
    "    block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_comment)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = PReLU()(block1)\n",
    "    block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = PReLU()(block1)\n",
    "\n",
    "    #we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n",
    "    #if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n",
    "    resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_comment)\n",
    "    resize_emb = PReLU()(resize_emb)\n",
    "    \n",
    "    block1_output = add([block1, resize_emb])\n",
    "    block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n",
    "\n",
    "    block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = PReLU()(block2)\n",
    "    block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = PReLU()(block2)\n",
    "    \n",
    "    block2_output = add([block2, block1_output])\n",
    "    block2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n",
    "\n",
    "    block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = PReLU()(block3)\n",
    "    block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = PReLU()(block3)\n",
    "    \n",
    "    block3_output = add([block3, block2_output])\n",
    "    block3_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output)\n",
    "\n",
    "    block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = PReLU()(block4)\n",
    "    block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block4)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = PReLU()(block4)\n",
    "\n",
    "    output = add([block4, block3_output])\n",
    "    output = GlobalMaxPooling1D()(output)\n",
    "    output = Dense(dense_nr, activation='linear')(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    output = Dense(6, activation='sigmoid')(output)\n",
    "    adam = optimizers.Adam(lr=0.0009)\n",
    "    \n",
    "\n",
    "    model = Model(comment, output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gK_-8Y5Ix75R"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BJlrGOHIjedQ"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Permute\n",
    "def get_gru_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    embed = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(embed)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x = Conv1D(60, kernel_size=3, padding='same', activation='linear')(x)\n",
    "    x = PReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    mx = GlobalMaxPool1D()(x)\n",
    "    ax = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    y = SpatialDropout1D(0.2)(embed)\n",
    "    y = Bidirectional(CuDNNGRU(128, return_sequences=True))(y)\n",
    "    y = Conv1D(60, kernel_size=3, padding='same', activation='linear')(y)\n",
    "    y = PReLU()(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    my = GlobalMaxPool1D()(y)\n",
    "    ay = GlobalAveragePooling1D()(y)\n",
    "    '''\n",
    "    #att = Attention()(x)\n",
    "    conc = concatenate([ax,mx])\n",
    "    conc = Permute((2, 1))(conc)\n",
    "    #conc = Flatten()(conc)\n",
    "    #conc = Dense(64)(conc)\n",
    "    #conc = Flatten()(conc)\n",
    "    #conc = Reshape((1, 64))(conc)\n",
    "    x = CuDNNGRU(64, return_sequences=True)(conc)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    #adam = optimizers.Adam(lr=0.001)\n",
    "    adam = optimizers.Nadam(lr=0.0002)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = optimizers.Adam(lr=0.0009)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-cnDyREuOBIG"
   },
   "outputs": [],
   "source": [
    "def train_folds(X, y, fold_count, model_list, model_name):\n",
    "      fold_size = len(X) // fold_count\n",
    "      models = []\n",
    "      total_meta = []\n",
    "      auc_list = []\n",
    "      for fold_id in range(0, fold_count):\n",
    "          print(\"FOLD {}\".format(fold_id))\n",
    "          fold_start = fold_size * fold_id\n",
    "          fold_end = fold_start + fold_size\n",
    "            \n",
    "          if fold_id == fold_count - 1:\n",
    "              fold_end = len(X)\n",
    "\n",
    "          train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "          train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "          val_x = X[fold_start:fold_end]\n",
    "          val_y = y[fold_start:fold_end]\n",
    "            \n",
    "          save_path = os.path.join('models' , '%s_model.h5' % (model_name + str(fold_id)))\n",
    "          callbacks = [\n",
    "          ModelCheckpoint(\n",
    "              save_path, save_best_only=True, verbose=False)\n",
    "          ]\n",
    "\n",
    "          model, best_auc = _train_model(model_list[fold_id], train_x, train_y, val_x, val_y,callbacks)\n",
    "          meta = model.predict(val_x, batch_size=128)\n",
    "          if (fold_id == 0):\n",
    "              total_meta = meta\n",
    "          else:\n",
    "              total_meta = np.concatenate((total_meta, meta), axis=0)\n",
    "          model_path = os.path.join('models', \"model{0}_weights.npy\".format(fold_id))\n",
    "          np.save(model_path, model.get_weights())\n",
    "          models.append(model)\n",
    "          auc_list.append(best_auc)\n",
    "\n",
    "      return models, total_meta, auc_list\n",
    "\n",
    "def _train_model(model, train_x, train_y, val_x, val_y, callbacks):\n",
    "    batch_size = 256\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    best_auc = -1\n",
    "    current_epoch = 0\n",
    "    #charCNN:LSTM\n",
    "    #train_x = np.reshape(train_x, train_x.shape + (1,))\n",
    "    #val_x = np.reshape(val_x, val_x.shape + (1,))\n",
    "    learning_rate = 0.001\n",
    "    while True:\n",
    "        if(current_epoch>0):\n",
    "            if(current_epoch==9):\n",
    "                learning_rate = learning_rate * (0.9**current_epoch)\n",
    "                K.set_value(model.optimizer.lr, learning_rate)\n",
    "            if(current_epoch==12):\n",
    "                learning_rate = learning_rate * (0.7**current_epoch)\n",
    "                K.set_value(model.optimizer.lr, learning_rate)\n",
    "            if(current_epoch==14):\n",
    "                learning_rate = learning_rate * (0.7**current_epoch)\n",
    "                K.set_value(model.optimizer.lr, learning_rate)\n",
    "        if(current_epoch>14):\n",
    "            if(current_epoch%3==0):\n",
    "                learning_rate = learning_rate * (0.7**current_epoch)\n",
    "                K.set_value(model.optimizer.lr, learning_rate)\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            validation_data=(val_x, val_y),\n",
    "            callbacks=callbacks,\n",
    "            verbose=2)\n",
    "        \n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "\n",
    "        total_loss = 0\n",
    "        total_auc = 0\n",
    "        for j in range(6):\n",
    "            loss = log_loss(val_y[:, j], y_pred[:, j])\n",
    "            auc = compute_auc(val_y[:, j], y_pred[:, j])\n",
    "            total_auc += auc\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= 6.\n",
    "        total_auc /= 6.\n",
    "\n",
    "        print(\"Epoch {0} logloss {1} best_logloss {2}, ROC_AUC {3}\".format(current_epoch, total_loss, best_loss, total_auc))\n",
    "\n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss < best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_auc = total_auc\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == 3:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model, best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F4DVcAHXSVb7"
   },
   "outputs": [],
   "source": [
    "def compute_auc(y_true, y_pred):\n",
    "  try:\n",
    "    return metrics.roc_auc_score(y_true, y_pred)\n",
    "  except ValueError:\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WPzTKe9riou3"
   },
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PaxLuXeWPOOV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/stgc/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_1 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_2 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_3 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_4 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_5 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_6 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_7 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_8 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_8 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "7\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_9 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_9 (Capsule)          (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "8\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 300, 300)          48000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_10 (Spatia (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 300, 200)          321600    \n",
      "_________________________________________________________________\n",
      "capsule_10 (Capsule)         (None, 10, 16)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 48,354,566\n",
      "Trainable params: 354,566\n",
      "Non-trainable params: 48,000,000\n",
      "_________________________________________________________________\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "list_models = []\n",
    "folds = 10\n",
    "for fold in range(0, folds):\n",
    "    model = get_cap_model()\n",
    "    list_models.append(model)\n",
    "    print(fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NxaGTK8rPZ7K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0799 - acc: 0.9764 - val_loss: 0.0495 - val_acc: 0.9819\n",
      "Epoch 0 logloss 0.0495378013668 best_logloss -1, ROC_AUC 0.959728137038\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0508 - acc: 0.9813 - val_loss: 0.0446 - val_acc: 0.9832\n",
      "Epoch 1 logloss 0.044607783214 best_logloss 0.0495378013668, ROC_AUC 0.980213413029\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0474 - acc: 0.9823 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Epoch 2 logloss 0.0425415083447 best_logloss 0.044607783214, ROC_AUC 0.982417590278\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 70s - loss: 0.0451 - acc: 0.9831 - val_loss: 0.0425 - val_acc: 0.9835\n",
      "Epoch 3 logloss 0.0425309616387 best_logloss 0.0425415083447, ROC_AUC 0.984296917247\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0434 - acc: 0.9835 - val_loss: 0.0404 - val_acc: 0.9840\n",
      "Epoch 4 logloss 0.040388903079 best_logloss 0.0425309616387, ROC_AUC 0.987125981344\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0419 - acc: 0.9838 - val_loss: 0.0414 - val_acc: 0.9835\n",
      "Epoch 5 logloss 0.0414490022516 best_logloss 0.040388903079, ROC_AUC 0.988483739616\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0407 - acc: 0.9843 - val_loss: 0.0393 - val_acc: 0.9843\n",
      "Epoch 6 logloss 0.0393261364749 best_logloss 0.040388903079, ROC_AUC 0.988492746838\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0396 - acc: 0.9847 - val_loss: 0.0395 - val_acc: 0.9841\n",
      "Epoch 7 logloss 0.0394510372902 best_logloss 0.0393261364749, ROC_AUC 0.989208475125\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0387 - acc: 0.9849 - val_loss: 0.0401 - val_acc: 0.9840\n",
      "Epoch 8 logloss 0.0400574090327 best_logloss 0.0393261364749, ROC_AUC 0.989005265082\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0387 - val_acc: 0.9845\n",
      "Epoch 9 logloss 0.0387243694856 best_logloss 0.0393261364749, ROC_AUC 0.989313660722\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0360 - acc: 0.9857 - val_loss: 0.0386 - val_acc: 0.9842\n",
      "Epoch 10 logloss 0.0386469006002 best_logloss 0.0387243694856, ROC_AUC 0.989506082863\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0357 - acc: 0.9859 - val_loss: 0.0395 - val_acc: 0.9840\n",
      "Epoch 11 logloss 0.0394600046912 best_logloss 0.0386469006002, ROC_AUC 0.989431363925\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0346 - acc: 0.9863 - val_loss: 0.0387 - val_acc: 0.9844\n",
      "Epoch 12 logloss 0.0387114107622 best_logloss 0.0386469006002, ROC_AUC 0.989477984166\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0346 - acc: 0.9863 - val_loss: 0.0386 - val_acc: 0.9845\n",
      "Epoch 13 logloss 0.0385776274403 best_logloss 0.0386469006002, ROC_AUC 0.989473467683\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0345 - acc: 0.9862 - val_loss: 0.0386 - val_acc: 0.9845\n",
      "Epoch 14 logloss 0.0385791242553 best_logloss 0.0385776274403, ROC_AUC 0.989472472195\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0345 - acc: 0.9864 - val_loss: 0.0386 - val_acc: 0.9845\n",
      "Epoch 15 logloss 0.0385791244582 best_logloss 0.0385776274403, ROC_AUC 0.989472472195\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0345 - acc: 0.9864 - val_loss: 0.0386 - val_acc: 0.9845\n",
      "Epoch 16 logloss 0.03857912447 best_logloss 0.0385776274403, ROC_AUC 0.989472472195\n",
      "FOLD 1\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0797 - acc: 0.9764 - val_loss: 0.0523 - val_acc: 0.9808\n",
      "Epoch 0 logloss 0.0523020104627 best_logloss -1, ROC_AUC 0.952259455054\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0499 - acc: 0.9816 - val_loss: 0.0467 - val_acc: 0.9822\n",
      "Epoch 1 logloss 0.0467278798128 best_logloss 0.0523020104627, ROC_AUC 0.979675867072\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0465 - acc: 0.9826 - val_loss: 0.0447 - val_acc: 0.9825\n",
      "Epoch 2 logloss 0.0446897329728 best_logloss 0.0467278798128, ROC_AUC 0.983356099485\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0445 - acc: 0.9831 - val_loss: 0.0435 - val_acc: 0.9832\n",
      "Epoch 3 logloss 0.043470981858 best_logloss 0.0446897329728, ROC_AUC 0.984656921751\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0433 - acc: 0.9834 - val_loss: 0.0420 - val_acc: 0.9835\n",
      "Epoch 4 logloss 0.0420394624975 best_logloss 0.043470981858, ROC_AUC 0.987344816242\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0416 - acc: 0.9840 - val_loss: 0.0428 - val_acc: 0.9832\n",
      "Epoch 5 logloss 0.0428094996486 best_logloss 0.0420394624975, ROC_AUC 0.987083560314\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0407 - acc: 0.9842 - val_loss: 0.0418 - val_acc: 0.9835\n",
      "Epoch 6 logloss 0.0417617707711 best_logloss 0.0420394624975, ROC_AUC 0.987078717282\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0397 - acc: 0.9845 - val_loss: 0.0418 - val_acc: 0.9837\n",
      "Epoch 7 logloss 0.0417770238937 best_logloss 0.0417617707711, ROC_AUC 0.988826233325\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0387 - acc: 0.9848 - val_loss: 0.0420 - val_acc: 0.9838\n",
      "Epoch 8 logloss 0.0420453663581 best_logloss 0.0417617707711, ROC_AUC 0.98819503363\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0367 - acc: 0.9856 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Epoch 9 logloss 0.0408991821767 best_logloss 0.0417617707711, ROC_AUC 0.989496460851\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0360 - acc: 0.9859 - val_loss: 0.0405 - val_acc: 0.9838\n",
      "Epoch 10 logloss 0.0405300961417 best_logloss 0.0408991821767, ROC_AUC 0.989624165846\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 77s - loss: 0.0357 - acc: 0.9859 - val_loss: 0.0412 - val_acc: 0.9837\n",
      "Epoch 11 logloss 0.0411796804632 best_logloss 0.0405300961417, ROC_AUC 0.990016554969\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0345 - acc: 0.9864 - val_loss: 0.0407 - val_acc: 0.9838\n",
      "Epoch 12 logloss 0.0406918313093 best_logloss 0.0405300961417, ROC_AUC 0.989967957953\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0346 - acc: 0.9863 - val_loss: 0.0406 - val_acc: 0.9839\n",
      "Epoch 13 logloss 0.0405840373983 best_logloss 0.0405300961417, ROC_AUC 0.989942805082\n",
      "FOLD 2\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 0.0785 - acc: 0.9766 - val_loss: 0.0493 - val_acc: 0.9817\n",
      "Epoch 0 logloss 0.049347487191 best_logloss -1, ROC_AUC 0.972391881346\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0509 - acc: 0.9814 - val_loss: 0.0448 - val_acc: 0.9830\n",
      "Epoch 1 logloss 0.0447789155181 best_logloss 0.049347487191, ROC_AUC 0.982403612728\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 0.0475 - acc: 0.9822 - val_loss: 0.0426 - val_acc: 0.9833\n",
      "Epoch 2 logloss 0.0425710531028 best_logloss 0.0447789155181, ROC_AUC 0.986307414307\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 0.0450 - acc: 0.9828 - val_loss: 0.0419 - val_acc: 0.9836\n",
      "Epoch 3 logloss 0.0418810076946 best_logloss 0.0425710531028, ROC_AUC 0.987870151353\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0433 - acc: 0.9834 - val_loss: 0.0398 - val_acc: 0.9843\n",
      "Epoch 4 logloss 0.0397723282781 best_logloss 0.0418810076946, ROC_AUC 0.989220903846\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0420 - acc: 0.9837 - val_loss: 0.0405 - val_acc: 0.9838\n",
      "Epoch 5 logloss 0.0405430641217 best_logloss 0.0397723282781, ROC_AUC 0.988903556424\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0407 - acc: 0.9841 - val_loss: 0.0394 - val_acc: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 logloss 0.0394066876439 best_logloss 0.0397723282781, ROC_AUC 0.989879967923\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0400 - acc: 0.9844 - val_loss: 0.0388 - val_acc: 0.9843\n",
      "Epoch 7 logloss 0.038814331639 best_logloss 0.0394066876439, ROC_AUC 0.990150902867\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0390 - val_acc: 0.9845\n",
      "Epoch 8 logloss 0.0389842737298 best_logloss 0.038814331639, ROC_AUC 0.990467128994\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0370 - acc: 0.9854 - val_loss: 0.0381 - val_acc: 0.9846\n",
      "Epoch 9 logloss 0.0381065866304 best_logloss 0.038814331639, ROC_AUC 0.990292525538\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0384 - val_acc: 0.9847\n",
      "Epoch 10 logloss 0.0383831184886 best_logloss 0.0381065866304, ROC_AUC 0.99037975392\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0360 - acc: 0.9858 - val_loss: 0.0384 - val_acc: 0.9845\n",
      "Epoch 11 logloss 0.0384305593597 best_logloss 0.0381065866304, ROC_AUC 0.99013114556\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0350 - acc: 0.9862 - val_loss: 0.0385 - val_acc: 0.9845\n",
      "Epoch 12 logloss 0.0385478615475 best_logloss 0.0381065866304, ROC_AUC 0.990329421876\n",
      "FOLD 3\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0806 - acc: 0.9762 - val_loss: 0.0506 - val_acc: 0.9817\n",
      "Epoch 0 logloss 0.0506370459616 best_logloss -1, ROC_AUC 0.956795483305\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 70s - loss: 0.0502 - acc: 0.9816 - val_loss: 0.0473 - val_acc: 0.9823\n",
      "Epoch 1 logloss 0.0472698313177 best_logloss 0.0506370459616, ROC_AUC 0.966730818117\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0467 - acc: 0.9825 - val_loss: 0.0442 - val_acc: 0.9833\n",
      "Epoch 2 logloss 0.0442369412899 best_logloss 0.0472698313177, ROC_AUC 0.979932683982\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0446 - acc: 0.9831 - val_loss: 0.0429 - val_acc: 0.9836\n",
      "Epoch 3 logloss 0.0428688636983 best_logloss 0.0442369412899, ROC_AUC 0.98173860476\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0428 - acc: 0.9834 - val_loss: 0.0419 - val_acc: 0.9842\n",
      "Epoch 4 logloss 0.041918686986 best_logloss 0.0428688636983, ROC_AUC 0.983490574283\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0419 - acc: 0.9838 - val_loss: 0.0420 - val_acc: 0.9840\n",
      "Epoch 5 logloss 0.0420230525702 best_logloss 0.041918686986, ROC_AUC 0.984970758365\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0403 - acc: 0.9844 - val_loss: 0.0413 - val_acc: 0.9845\n",
      "Epoch 6 logloss 0.0412596219016 best_logloss 0.041918686986, ROC_AUC 0.983999342609\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0410 - val_acc: 0.9846\n",
      "Epoch 7 logloss 0.0410452977801 best_logloss 0.0412596219016, ROC_AUC 0.985007038045\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0385 - acc: 0.9849 - val_loss: 0.0422 - val_acc: 0.9835\n",
      "Epoch 8 logloss 0.0422435095499 best_logloss 0.0410452977801, ROC_AUC 0.984427311207\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0406 - val_acc: 0.9845\n",
      "Epoch 9 logloss 0.0405689204253 best_logloss 0.0410452977801, ROC_AUC 0.985373705631\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0361 - acc: 0.9857 - val_loss: 0.0408 - val_acc: 0.9844\n",
      "Epoch 10 logloss 0.0407829085232 best_logloss 0.0405689204253, ROC_AUC 0.985801681715\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0355 - acc: 0.9859 - val_loss: 0.0417 - val_acc: 0.9840\n",
      "Epoch 11 logloss 0.0416657861492 best_logloss 0.0405689204253, ROC_AUC 0.986693192037\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0347 - acc: 0.9862 - val_loss: 0.0410 - val_acc: 0.9844\n",
      "Epoch 12 logloss 0.0409703544315 best_logloss 0.0405689204253, ROC_AUC 0.986504512659\n",
      "FOLD 4\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0781 - acc: 0.9765 - val_loss: 0.0489 - val_acc: 0.9820\n",
      "Epoch 0 logloss 0.0489047349419 best_logloss -1, ROC_AUC 0.967197440947\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0507 - acc: 0.9813 - val_loss: 0.0452 - val_acc: 0.9828\n",
      "Epoch 1 logloss 0.045211836374 best_logloss 0.0489047349419, ROC_AUC 0.979310746387\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0470 - acc: 0.9824 - val_loss: 0.0438 - val_acc: 0.9832\n",
      "Epoch 2 logloss 0.0438179187609 best_logloss 0.045211836374, ROC_AUC 0.981708387429\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0449 - acc: 0.9828 - val_loss: 0.0421 - val_acc: 0.9838\n",
      "Epoch 3 logloss 0.0420881149601 best_logloss 0.0438179187609, ROC_AUC 0.983744888801\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0431 - acc: 0.9833 - val_loss: 0.0413 - val_acc: 0.9839\n",
      "Epoch 4 logloss 0.0412878707543 best_logloss 0.0420881149601, ROC_AUC 0.982953237168\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0418 - acc: 0.9837 - val_loss: 0.0402 - val_acc: 0.9840\n",
      "Epoch 5 logloss 0.0402164607898 best_logloss 0.0412878707543, ROC_AUC 0.98523543036\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0408 - acc: 0.9841 - val_loss: 0.0400 - val_acc: 0.9846\n",
      "Epoch 6 logloss 0.0400324829641 best_logloss 0.0402164607898, ROC_AUC 0.985225696432\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0399 - acc: 0.9844 - val_loss: 0.0398 - val_acc: 0.9843\n",
      "Epoch 7 logloss 0.0397582652081 best_logloss 0.0400324829641, ROC_AUC 0.986579444922\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0390 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9845\n",
      "Epoch 8 logloss 0.0395348934865 best_logloss 0.0397582652081, ROC_AUC 0.986211974412\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0391 - val_acc: 0.9849\n",
      "Epoch 9 logloss 0.0390574724583 best_logloss 0.0395348934865, ROC_AUC 0.986654511067\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0364 - acc: 0.9856 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 10 logloss 0.0391776223851 best_logloss 0.0390574724583, ROC_AUC 0.986739328252\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0359 - acc: 0.9857 - val_loss: 0.0394 - val_acc: 0.9842\n",
      "Epoch 11 logloss 0.0394409961367 best_logloss 0.0390574724583, ROC_AUC 0.987872156293\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0350 - acc: 0.9860 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "Epoch 12 logloss 0.0388058800969 best_logloss 0.0390574724583, ROC_AUC 0.98779983937\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0348 - acc: 0.9861 - val_loss: 0.0387 - val_acc: 0.9846\n",
      "Epoch 13 logloss 0.0387284553241 best_logloss 0.0388058800969, ROC_AUC 0.98764231019\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0347 - acc: 0.9862 - val_loss: 0.0387 - val_acc: 0.9846\n",
      "Epoch 14 logloss 0.038728105878 best_logloss 0.0387284553241, ROC_AUC 0.987641953541\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0348 - acc: 0.9862 - val_loss: 0.0387 - val_acc: 0.9846\n",
      "Epoch 15 logloss 0.0387281059215 best_logloss 0.038728105878, ROC_AUC 0.987641953541\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0347 - acc: 0.9862 - val_loss: 0.0387 - val_acc: 0.9846\n",
      "Epoch 16 logloss 0.0387281060581 best_logloss 0.038728105878, ROC_AUC 0.987641953541\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 0.0348 - acc: 0.9861 - val_loss: 0.0387 - val_acc: 0.9846\n",
      "Epoch 17 logloss 0.0387281060332 best_logloss 0.038728105878, ROC_AUC 0.987641953541\n",
      "FOLD 5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 73s - loss: 0.0788 - acc: 0.9764 - val_loss: 0.0477 - val_acc: 0.9828\n",
      "Epoch 0 logloss 0.0476729884476 best_logloss -1, ROC_AUC 0.963104462711\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0510 - acc: 0.9814 - val_loss: 0.0443 - val_acc: 0.9837\n",
      "Epoch 1 logloss 0.044293622651 best_logloss 0.0476729884476, ROC_AUC 0.977690071137\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0475 - acc: 0.9823 - val_loss: 0.0419 - val_acc: 0.9841\n",
      "Epoch 2 logloss 0.0419059875459 best_logloss 0.044293622651, ROC_AUC 0.982377570088\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0449 - acc: 0.9831 - val_loss: 0.0405 - val_acc: 0.9846\n",
      "Epoch 3 logloss 0.0405127221348 best_logloss 0.0419059875459, ROC_AUC 0.984234832206\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0433 - acc: 0.9833 - val_loss: 0.0399 - val_acc: 0.9848\n",
      "Epoch 4 logloss 0.0398745293879 best_logloss 0.0405127221348, ROC_AUC 0.984699857101\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0421 - acc: 0.9836 - val_loss: 0.0393 - val_acc: 0.9847\n",
      "Epoch 5 logloss 0.0392997528155 best_logloss 0.0398745293879, ROC_AUC 0.9854507668\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0408 - acc: 0.9843 - val_loss: 0.0415 - val_acc: 0.9834\n",
      "Epoch 6 logloss 0.0415175498562 best_logloss 0.0392997528155, ROC_AUC 0.986957991517\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0397 - acc: 0.9845 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 7 logloss 0.0391722577837 best_logloss 0.0392997528155, ROC_AUC 0.986569315605\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0399 - val_acc: 0.9841\n",
      "Epoch 8 logloss 0.0398724345976 best_logloss 0.0391722577837, ROC_AUC 0.987601302318\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0371 - acc: 0.9855 - val_loss: 0.0387 - val_acc: 0.9844\n",
      "Epoch 9 logloss 0.038698159405 best_logloss 0.0391722577837, ROC_AUC 0.987644393159\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0393 - val_acc: 0.9842\n",
      "Epoch 10 logloss 0.0393000468451 best_logloss 0.038698159405, ROC_AUC 0.98736251601\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0359 - acc: 0.9858 - val_loss: 0.0390 - val_acc: 0.9847\n",
      "Epoch 11 logloss 0.0390237287339 best_logloss 0.038698159405, ROC_AUC 0.98688248329\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0357 - acc: 0.9860 - val_loss: 0.0390 - val_acc: 0.9845\n",
      "Epoch 12 logloss 0.0389660107128 best_logloss 0.038698159405, ROC_AUC 0.987326020283\n",
      "FOLD 6\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0819 - acc: 0.9756 - val_loss: 0.0511 - val_acc: 0.9817\n",
      "Epoch 0 logloss 0.0511022560485 best_logloss -1, ROC_AUC 0.965174487551\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 74s - loss: 0.0512 - acc: 0.9815 - val_loss: 0.0458 - val_acc: 0.9828\n",
      "Epoch 1 logloss 0.0458276049556 best_logloss 0.0511022560485, ROC_AUC 0.976283012409\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0473 - acc: 0.9823 - val_loss: 0.0434 - val_acc: 0.9835\n",
      "Epoch 2 logloss 0.0433711631958 best_logloss 0.0458276049556, ROC_AUC 0.980859643216\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0450 - acc: 0.9829 - val_loss: 0.0426 - val_acc: 0.9838\n",
      "Epoch 3 logloss 0.0426107868811 best_logloss 0.0433711631958, ROC_AUC 0.985582657266\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0434 - acc: 0.9833 - val_loss: 0.0416 - val_acc: 0.9840\n",
      "Epoch 4 logloss 0.0416156576314 best_logloss 0.0426107868811, ROC_AUC 0.985332445771\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0419 - acc: 0.9838 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 5 logloss 0.0404639154048 best_logloss 0.0416156576314, ROC_AUC 0.987184545607\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0407 - acc: 0.9842 - val_loss: 0.0401 - val_acc: 0.9844\n",
      "Epoch 6 logloss 0.0401481672166 best_logloss 0.0404639154048, ROC_AUC 0.986340830576\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0397 - acc: 0.9844 - val_loss: 0.0429 - val_acc: 0.9831\n",
      "Epoch 7 logloss 0.0428639543988 best_logloss 0.0401481672166, ROC_AUC 0.987861470143\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0388 - acc: 0.9848 - val_loss: 0.0397 - val_acc: 0.9845\n",
      "Epoch 8 logloss 0.0396742345172 best_logloss 0.0401481672166, ROC_AUC 0.988609038446\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0369 - acc: 0.9854 - val_loss: 0.0396 - val_acc: 0.9842\n",
      "Epoch 9 logloss 0.0396194597064 best_logloss 0.0396742345172, ROC_AUC 0.988439178784\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0362 - acc: 0.9857 - val_loss: 0.0407 - val_acc: 0.9837\n",
      "Epoch 10 logloss 0.0407040857895 best_logloss 0.0396194597064, ROC_AUC 0.988545196423\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0357 - acc: 0.9858 - val_loss: 0.0394 - val_acc: 0.9844\n",
      "Epoch 11 logloss 0.0394288724804 best_logloss 0.0396194597064, ROC_AUC 0.988995829282\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0348 - acc: 0.9861 - val_loss: 0.0396 - val_acc: 0.9844\n",
      "Epoch 12 logloss 0.0396457491431 best_logloss 0.0394288724804, ROC_AUC 0.988911709111\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0349 - acc: 0.9861 - val_loss: 0.0397 - val_acc: 0.9843\n",
      "Epoch 13 logloss 0.0396518014238 best_logloss 0.0394288724804, ROC_AUC 0.988899058374\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0348 - acc: 0.9862 - val_loss: 0.0397 - val_acc: 0.9843\n",
      "Epoch 14 logloss 0.0396523008973 best_logloss 0.0394288724804, ROC_AUC 0.988898811621\n",
      "FOLD 7\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0814 - acc: 0.9757 - val_loss: 0.0507 - val_acc: 0.9816\n",
      "Epoch 0 logloss 0.0507220162918 best_logloss -1, ROC_AUC 0.971983635783\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0509 - acc: 0.9815 - val_loss: 0.0466 - val_acc: 0.9823\n",
      "Epoch 1 logloss 0.0466318336051 best_logloss 0.0507220162918, ROC_AUC 0.974761043046\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0470 - acc: 0.9823 - val_loss: 0.0450 - val_acc: 0.9825\n",
      "Epoch 2 logloss 0.045047812106 best_logloss 0.0466318336051, ROC_AUC 0.982157904128\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 0.0448 - acc: 0.9830 - val_loss: 0.0430 - val_acc: 0.9831\n",
      "Epoch 3 logloss 0.043031733213 best_logloss 0.045047812106, ROC_AUC 0.98328818696\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 0.0433 - acc: 0.9834 - val_loss: 0.0424 - val_acc: 0.9834\n",
      "Epoch 4 logloss 0.0424473466197 best_logloss 0.043031733213, ROC_AUC 0.985075692483\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 0.0417 - acc: 0.9840 - val_loss: 0.0425 - val_acc: 0.9832\n",
      "Epoch 5 logloss 0.0424686225026 best_logloss 0.0424473466197, ROC_AUC 0.986208695378\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0405 - acc: 0.9842 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 6 logloss 0.0419458135266 best_logloss 0.0424473466197, ROC_AUC 0.986613291695\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0410 - val_acc: 0.9835\n",
      "Epoch 7 logloss 0.0409931199978 best_logloss 0.0419458135266, ROC_AUC 0.986830020025\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0386 - acc: 0.9847 - val_loss: 0.0417 - val_acc: 0.9837\n",
      "Epoch 8 logloss 0.0416928323276 best_logloss 0.0409931199978, ROC_AUC 0.987141406453\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0364 - acc: 0.9856 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 9 logloss 0.0411703607065 best_logloss 0.0409931199978, ROC_AUC 0.987588996683\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 72s - loss: 0.0360 - acc: 0.9858 - val_loss: 0.0410 - val_acc: 0.9839\n",
      "Epoch 10 logloss 0.0410038437409 best_logloss 0.0409931199978, ROC_AUC 0.987487488535\n",
      "FOLD 8\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0817 - acc: 0.9761 - val_loss: 0.0534 - val_acc: 0.9804\n",
      "Epoch 0 logloss 0.0534341054068 best_logloss -1, ROC_AUC 0.975144872342\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0507 - acc: 0.9814 - val_loss: 0.0440 - val_acc: 0.9832\n",
      "Epoch 1 logloss 0.043992087065 best_logloss 0.0534341054068, ROC_AUC 0.981100222563\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0473 - acc: 0.9823 - val_loss: 0.0439 - val_acc: 0.9833\n",
      "Epoch 2 logloss 0.0438561182961 best_logloss 0.043992087065, ROC_AUC 0.984390167501\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0452 - acc: 0.9829 - val_loss: 0.0410 - val_acc: 0.9842\n",
      "Epoch 3 logloss 0.0410261343047 best_logloss 0.0438561182961, ROC_AUC 0.986500255338\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0433 - acc: 0.9835 - val_loss: 0.0405 - val_acc: 0.9844\n",
      "Epoch 4 logloss 0.0405252409889 best_logloss 0.0410261343047, ROC_AUC 0.987585915771\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0422 - acc: 0.9838 - val_loss: 0.0406 - val_acc: 0.9842\n",
      "Epoch 5 logloss 0.0406392410149 best_logloss 0.0405252409889, ROC_AUC 0.987709293928\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0396 - val_acc: 0.9847\n",
      "Epoch 6 logloss 0.0395540211893 best_logloss 0.0405252409889, ROC_AUC 0.988622439628\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0399 - acc: 0.9843 - val_loss: 0.0395 - val_acc: 0.9847\n",
      "Epoch 7 logloss 0.0395373712303 best_logloss 0.0395540211893, ROC_AUC 0.988062449267\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0391 - acc: 0.9846 - val_loss: 0.0392 - val_acc: 0.9850\n",
      "Epoch 8 logloss 0.0391726208811 best_logloss 0.0395373712303, ROC_AUC 0.988271988951\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0369 - acc: 0.9855 - val_loss: 0.0386 - val_acc: 0.9848\n",
      "Epoch 9 logloss 0.0385972953824 best_logloss 0.0391726208811, ROC_AUC 0.988605614033\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0362 - acc: 0.9856 - val_loss: 0.0387 - val_acc: 0.9850\n",
      "Epoch 10 logloss 0.0387177688005 best_logloss 0.0385972953824, ROC_AUC 0.988412839526\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0358 - acc: 0.9857 - val_loss: 0.0386 - val_acc: 0.9855\n",
      "Epoch 11 logloss 0.038641692279 best_logloss 0.0385972953824, ROC_AUC 0.98814460109\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0348 - acc: 0.9861 - val_loss: 0.0386 - val_acc: 0.9852\n",
      "Epoch 12 logloss 0.0386166214572 best_logloss 0.0385972953824, ROC_AUC 0.988316710946\n",
      "FOLD 9\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 73s - loss: 0.0802 - acc: 0.9760 - val_loss: 0.0497 - val_acc: 0.9818\n",
      "Epoch 0 logloss 0.0496686428946 best_logloss -1, ROC_AUC 0.968170696912\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0504 - acc: 0.9817 - val_loss: 0.0464 - val_acc: 0.9822\n",
      "Epoch 1 logloss 0.0464349622106 best_logloss 0.0496686428946, ROC_AUC 0.978650669575\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0469 - acc: 0.9824 - val_loss: 0.0461 - val_acc: 0.9820\n",
      "Epoch 2 logloss 0.046053086852 best_logloss 0.0464349622106, ROC_AUC 0.979173437479\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0446 - acc: 0.9832 - val_loss: 0.0429 - val_acc: 0.9836\n",
      "Epoch 3 logloss 0.0429333460315 best_logloss 0.046053086852, ROC_AUC 0.983766545001\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0429 - acc: 0.9836 - val_loss: 0.0422 - val_acc: 0.9836\n",
      "Epoch 4 logloss 0.0422282092316 best_logloss 0.0429333460315, ROC_AUC 0.98660117367\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0412 - acc: 0.9841 - val_loss: 0.0415 - val_acc: 0.9839\n",
      "Epoch 5 logloss 0.0414778862553 best_logloss 0.0422282092316, ROC_AUC 0.986318467963\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0402 - acc: 0.9844 - val_loss: 0.0442 - val_acc: 0.9828\n",
      "Epoch 6 logloss 0.0441549298907 best_logloss 0.0414778862553, ROC_AUC 0.987878623755\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0395 - acc: 0.9846 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "Epoch 7 logloss 0.0415685166192 best_logloss 0.0414778862553, ROC_AUC 0.988423856082\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0387 - acc: 0.9850 - val_loss: 0.0413 - val_acc: 0.9840\n",
      "Epoch 8 logloss 0.041292135834 best_logloss 0.0414778862553, ROC_AUC 0.988637498426\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0366 - acc: 0.9856 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 9 logloss 0.0405424561617 best_logloss 0.041292135834, ROC_AUC 0.989164376626\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0360 - acc: 0.9858 - val_loss: 0.0410 - val_acc: 0.9841\n",
      "Epoch 10 logloss 0.0410067634339 best_logloss 0.0405424561617, ROC_AUC 0.989447528624\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0356 - acc: 0.9860 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Epoch 11 logloss 0.0402914115921 best_logloss 0.0405424561617, ROC_AUC 0.989639843264\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 71s - loss: 0.0344 - acc: 0.9863 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "Epoch 12 logloss 0.0403283685694 best_logloss 0.0402914115921, ROC_AUC 0.989621923105\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0344 - acc: 0.9863 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Epoch 13 logloss 0.0403353588941 best_logloss 0.0402914115921, ROC_AUC 0.98959819006\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 72s - loss: 0.0343 - acc: 0.9864 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Epoch 14 logloss 0.0403362928842 best_logloss 0.0402914115921, ROC_AUC 0.989598688557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import auc\n",
    "from sklearn import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'capsule_2'\n",
    "models, total_meta, auc_list = train_folds(x_train, y_train, folds, list_models, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sIz1LQBKS215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained!\n",
      "Predicting results...\n",
      "predicted !\n",
      "Meta predicted !\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "print('Model trained!')\n",
    "print(\"Predicting results...\")\n",
    "\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    model_path = os.path.join('models', \"model{0}_weights.npy\".format(fold_id))\n",
    "    np.save(model_path, model.get_weights())\n",
    "        \n",
    "    test_predicts_path = os.path.join('models', \"test_predicts{0}.npy\".format(fold_id))\n",
    "    test_predicts = model.predict(x_test, batch_size=256)\n",
    "    test_predicts_list.append(test_predicts)\n",
    "    np.save(test_predicts_path, test_predicts)\n",
    "\n",
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "test_ids = test[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "test_predicts.to_csv('ori_pred_cap_lstm_glove.csv', index=False)\n",
    "print('predicted !')\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "subm = pd.read_csv('train_translated_sp_clean.csv')\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "total_meta_data = pd.concat([submid, pd.DataFrame(total_meta, columns = label_cols)], axis=1)\n",
    "total_meta_data.to_csv('ori_cap_lstm_glove_meta.csv', index=False)\n",
    "auc_folds = pd.DataFrame(data=auc_list)\n",
    "auc_folds.to_csv('auc_ori_cap_lstm_glove.csv', index=False)\n",
    "print('Meta predicted !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "J_J-V7HTiIeH"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lEOUXqNhKat5"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "conv.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "19sIA-ZSeNgUczg00uVgGxiGKWxsKmaZk",
     "timestamp": 1520847313061
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
