{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5c86b9bc-a478-4da9-adcc-b97ca0fbd0c9",
    "_uuid": "53d59528c17da4c1d9759786326a82d0c8765804"
   },
   "source": [
    "# Using FastText models (not vectors) for robust embeddings\n",
    "\n",
    "I'd like to explain my approach of using pretrained FastText models as input to Keras Neural Networks. FastText is a word embedding not unlike Word2Vec or GloVe, but the cool thing is that each word vector is based on sub-word character n-grams. That means that even for previously unseen words (e.g. due to typos), the model can make an educated guess towards its meaning. To find out more about FastText, check out both their [Github](https://github.com/facebookresearch/fastText/) and [website](https://fasttext.cc/).\n",
    "\n",
    "To do this, we won't be using the classic Keras embedding layer and instead hand-craft the embedding for each example. As a result, we need to  write more code and invest some time into preprocessing, but that is easily justified by the results.\n",
    "\n",
    "**Disclaimer: Loading the FastText model will take some serious memory! I recommend having at least 60 GB of RAM. EC2's p2.xlarge instance should have no problems with this, but you can always [add some swap](https://stackoverflow.com/questions/17173972/how-do-you-add-swap-to-an-ec2-instance) for good measure. I also added a section below to build a training generator for this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b88f84eb-051c-4fe8-8fc0-70365a0b9693",
    "_uuid": "168f1efdabebb51c2922aab58231921599f9348f"
   },
   "source": [
    "## Preparations: Getting FastText and the model\n",
    "\n",
    "First, build FastText from sources as described [here](https://github.com/facebookresearch/fastText#requirements). Don't worry, there's nothing crazy you have to do and it will finish in less than a minute. Next, install the Python package in your virtualenv following [these instructions](https://github.com/facebookresearch/fastText/tree/master/python).\n",
    "\n",
    "For the model, I use the one pretrained on English Wikipedia. I'd love to have one trained on Twitter or similar, since it might be more internet-slangy, but I haven't found any yet and don't feel like pretraining one myself. Download the model [here](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md). Make sure you get the bin, not just the vec (text) file. I'll assume you placed it (or a symlink to it) into your code directory and named it `ft_model.bin`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eea9d34b-3205-421f-bfd8-6e29f8092ddd",
    "_uuid": "e4542f71483666b9168dacc6949bcbaff8b66642"
   },
   "source": [
    "## Preparations: Exploring the model\n",
    "\n",
    "Let's explore the model! Go to your FastText directory and run `./fasttext nn <path_to_ft_model>`. Now you can enter some terms and see the nearest neighbors to this word in the embedding space. Here are some examples:\n",
    "\n",
    "```\n",
    "Query word? queen\n",
    "—queen 0.719091\n",
    "‘queen 0.692849\n",
    "#queen 0.656498\n",
    "queena 0.650313\n",
    "king 0.64931\n",
    "queen`s 0.63954\n",
    "king/queen 0.634855\n",
    "s/queen 0.627386\n",
    "princess 0.623889\n",
    "queeny 0.620919\n",
    "```\n",
    "\n",
    "Ok that looks pretty ugly. I suppose Facebook was not very exact in their cleaning of the input data. But some sensible suggestions are there: `king` and `princess`! Let's try a typo that is unlikely to have appeared in the original data:\n",
    "\n",
    "```\n",
    "Query word? dimensionnallity\n",
    "dimension, 0.722278\n",
    "dimensionality 0.708645\n",
    "dimensionful 0.698573\n",
    "codimension 0.689754\n",
    "codimensions 0.67555\n",
    "twodimensional 0.674745\n",
    "dimension 0.67258\n",
    "\\,kdimensional 0.668848\n",
    "‘dimensions 0.665725\n",
    "two–dimensional 0.665109\n",
    "```\n",
    "\n",
    "Sweet! Even though it has never seen that word, it recognizes it to be related with \"dimensionality\". Let's try some something mean:\n",
    "\n",
    "```\n",
    "Query word? dumb\n",
    "stupid 0.746051\n",
    "dumber 0.732965\n",
    "clueless 0.662594\n",
    "idiotic 0.64993\n",
    "silly 0.632314\n",
    "stupidstitious 0.628875\n",
    "stupidly 0.622968\n",
    "moronic 0.621633\n",
    "ignorant 0.620475\n",
    "stupider 0.617377\n",
    "```\n",
    "\n",
    "Nice! Even though this was trained on Wikipedia, we're getting at least some basic insults. I'll leave it to you to explore the really hateful words. They all seem to be there ;)\n",
    "\n",
    "**Note:** Keep in mind that exploring the nearest neighbors is a very superficial approach to understanding the model! The embedding space has 300 dimensions, and we boil them down to a single distance metric. We can't be sure in which dimensions these words are related to each other, but we can trust in the model to have learnt something sensible.\n",
    "\n",
    "**Pro tip:** Our data should be cleaned and normalized in a similar way as Facebook did before they trained this model. We can query the model to get some insights into what they did, e.g.\n",
    "\n",
    "```\n",
    "Query word? 1\n",
    "insel 0.483141\n",
    "inseln 0.401125\n",
    "...\n",
    "Query word? one\n",
    "two 0.692744\n",
    "three 0.676568\n",
    "...\n",
    "```\n",
    "\n",
    "This tells us they converted all numbers to their text equivalent, and so should we!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2f03f7c4-ecc8-47d3-8c59-9e4482e7684a",
    "_uuid": "ae7a32ce6f66748656faecde4d079857b992ac1e"
   },
   "source": [
    "## Loading and cleaning the data\n",
    "\n",
    "We define a method `normalize` to clean and prepare a single string. We will use it later to prepare our string data. Also, we load the data as we're used to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import cPickle\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras import regularizers\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import CuDNNGRU\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.models import Sequential\n",
    "import nltk\n",
    "from keras.optimizers import Nadam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "423398cb-482b-4dc3-9904-82c2d17d2e2c",
    "_kg_hide-output": true,
    "_uuid": "fa74d030d08aff58c32455cd4218d9ff0ef494d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastText import load_model\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNGRU\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "window_length = 200 # The amount of words we look at per example. Experiment with this.\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"\n",
    "    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Replace ips\n",
    "    s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n",
    "    # Isolate punctuation\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Replace numbers and symbols with language\n",
    "    s = s.replace('&', ' and ')\n",
    "    s = s.replace('@', ' at ')\n",
    "    s = s.replace('0', ' zero ')\n",
    "    s = s.replace('1', ' one ')\n",
    "    s = s.replace('2', ' two ')\n",
    "    s = s.replace('3', ' three ')\n",
    "    s = s.replace('4', ' four ')\n",
    "    s = s.replace('5', ' five ')\n",
    "    s = s.replace('6', ' six ')\n",
    "    s = s.replace('7', ' seven ')\n",
    "    s = s.replace('8', ' eight ')\n",
    "    s = s.replace('9', ' nine ')\n",
    "    return s\n",
    "\n",
    "print('\\nLoading data')\n",
    "train = pd.read_csv('cleaned_final_train_clean.csv')\n",
    "test = pd.read_csv('cleaned_test_clean.csv')\n",
    "train['comment_text'] = train['comment_text'].fillna('_empty_')\n",
    "test['comment_text'] = test['comment_text'].fillna('_empty_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n",
    "                        stop_words= 'english',ngram_range=(1,3),dtype=np.float32)\n",
    "vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n",
    "                        stop_words= 'english',ngram_range=(3,6),dtype=np.float32)\n",
    "\n",
    "# Word ngram vector\n",
    "tr_vect = vect_word.fit_transform(train['comment_text'])\n",
    "ts_vect = vect_word.transform(test['comment_text'])\n",
    "\n",
    "# Character n gram vector\n",
    "tr_vect_char = vect_char.fit_transform(train['comment_text'])\n",
    "ts_vect_char = vect_char.transform(test['comment_text'])\n",
    "\n",
    "\n",
    "X = sparse.hstack([tr_vect, tr_vect_char])\n",
    "x_test = sparse.hstack([ts_vect, ts_vect_char])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-02-25 02:32:20--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.20.137\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.20.137|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10356881291 (9.6G) [application/zip]\n",
      "Saving to: ‘wiki.en.zip’\n",
      "\n",
      "wiki.en.zip         100%[===================>]   9.65G  8.10MB/s    in 25m 35s \n",
      "\n",
      "2018-02-25 02:57:56 (6.44 MB/s) - ‘wiki.en.zip’ saved [10356881291/10356881291]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d98e8058-b635-408f-98ca-c0b999bc310c",
    "_uuid": "a4099da988bbe670ee7b389071e924ca1891cec2"
   },
   "source": [
    "Ok next, let's load the FastText model and define methods that convert text to a sequence of vectors. Note that I'm just considering the last n words of each text. You could play with this, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "5792a3ad-8b04-435f-bd3d-f54d7449921b",
    "_kg_hide-output": true,
    "_uuid": "d5a8656a2cb0b9cde191230f477faa17934180c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FT model\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    "]\n",
    "\n",
    "print('\\nLoading FT model')\n",
    "ft_model = load_model('wiki.en.bin')\n",
    "n_features = ft_model.get_dimension()\n",
    "\n",
    "def text_to_vector(text):\n",
    "    \"\"\"\n",
    "    Given a string, normalizes it, then splits it into words and finally converts\n",
    "    it to a sequence of word vectors.\n",
    "    \"\"\"\n",
    "    text = normalize(text)\n",
    "    words = text.split()\n",
    "    window = words[-window_length:]\n",
    "    \n",
    "    x = np.zeros((window_length, n_features))\n",
    "\n",
    "    for i, word in enumerate(window):\n",
    "        x[i, :] = ft_model.get_word_vector(word).astype('float32')\n",
    "\n",
    "    return x\n",
    "\n",
    "def df_to_data(df):\n",
    "    \"\"\"\n",
    "    Convert a given dataframe to a dataset of inputs for the NN.\n",
    "    \"\"\"\n",
    "    x = np.zeros((len(df), window_length, n_features), dtype='float32')\n",
    "\n",
    "    for i, comment in enumerate(df['comment_text'].values):\n",
    "        x[i, :] = text_to_vector(comment)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d9bc312a-e444-4058-b4ac-3f30d3d98940",
    "_uuid": "ae35399a098729b8e68d76a953d4c097917438bb"
   },
   "source": [
    "To convert an input dataframe to an input vector, just call `df_to_data`. This will result in the shape `(n_examples, window_length, n_features)`. Here, for each row we would have 200 words a 300 features each.\n",
    "\n",
    "**EDIT/NOTE:** This will probably not fit into your memory, so don't bother executing it :) Instead, read my generator guide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "1c98dd07-36b1-4b7f-b806-6a5d8bdabae1",
    "_kg_hide-output": true,
    "_uuid": "6a34513a82b5140d5e5c258f5c6427b83ae62245"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-245fc7ea9dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-802b0f01bc1a>\u001b[0m in \u001b[0;36mdf_to_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mConvert\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mof\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train = df_to_data(train)\n",
    "y_train = train[classes].values\n",
    "\n",
    "x_test = df_to_data(test)\n",
    "y_test = test[classes].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f582bb5-ac4b-4b67-8e34-6fa022416e57",
    "_uuid": "7b41f089ef0d93d41d565e99b03c0f19b30f26ce"
   },
   "source": [
    "And now you should be good to go! Train this as usual. You don't need an `EmbeddingLayer`, but you need to pass `input_shape=(window_length, n_features)` to the first layer in your NN.\n",
    "\n",
    "I'm still in the process of experimenting, but I already achieved a single-model LB score of `0.9842` with something very simple. Bagging multiple of these models got me into the top 100 easily. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4bb7ba9a-7c32-43db-9c52-f4fca204ab12",
    "_uuid": "4dfea6e85ca933626a14ce2df8e937051b863d0a"
   },
   "source": [
    "### PS: Using a generator so you don't have to keep the whole damn thing in memory\n",
    "As @liujilong pointed out, not even the p2.xlarge machine with 64 GB can hold both the training and test set for window sizes longer than ~100 words. It seems I underestimated how much memory this monster model eats! Also, locally I had long [added swap space](https://stackoverflow.com/questions/17173972/how-do-you-add-swap-to-an-ec2-instance) and switched to generators so I wouldn't have to keep the whole thing memory. Let me show you how to implement the generator part. This is also useful to add some randomization later on.\n",
    "\n",
    "The idea is that instead of converting the whole training set to one large array, we can write a function that just spits out one batch of data at a time, infinitely. Keras can automaticaly spin up a separate thread for this method (note though that \"threads\" in Python are ridiculous and do not give any speedup whatsoever). This means that we have to write some more code and training will be slightly slower, but we need only a fraction of the memory and we can add some cool randomization to each batch later on (see ideas section below).\n",
    "\n",
    "We can keep all the code from above. This generator method works only for training data, not for validation data, so you will need to split by hand. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f71c2d37-2a1e-483a-af4c-5e7c820ad29c",
    "_kg_hide-output": true,
    "_uuid": "74becdb6b32d5676e1f4a0d67b38a0ba355dac97"
   },
   "outputs": [],
   "source": [
    "# Split the dataset:\n",
    "#split_index = round(len(train) * 0.9)\n",
    "#shuffled_train = train.sample(frac=1)\n",
    "#df_train = shuffled_train.iloc[:split_index]\n",
    "#df_val = shuffled_train.iloc[split_index:]\n",
    "\n",
    "# Convert validation set to fixed array\n",
    "#x_val = df_to_data(df_val)\n",
    "#y_val = df_val[classes].values\n",
    "\n",
    "def data_generator(df, batch_size):\n",
    "    \"\"\"\n",
    "    Given a raw dataframe, generates infinite batches of FastText vectors.\n",
    "    \"\"\"\n",
    "    batch_i = 0 # Counter inside the current batch vector\n",
    "    batch_x = None # The current batch's x data\n",
    "    batch_y = None # The current batch's y data\n",
    "    \n",
    "    while True: # Loop forever\n",
    "        df = df.sample(frac=1) # Shuffle df each epoch\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            comment = row['comment_text']\n",
    "            \n",
    "            if batch_x is None:\n",
    "                batch_x = np.zeros((batch_size, window_length, n_features), dtype='float32')\n",
    "                batch_y = np.zeros((batch_size, len(classes)), dtype='float32')\n",
    "                \n",
    "            batch_x[batch_i] = text_to_vector(comment)\n",
    "            batch_y[batch_i] = row[classes].values\n",
    "            batch_i += 1\n",
    "\n",
    "            if batch_i == batch_size:\n",
    "                # Ready to yield the batch\n",
    "                yield batch_x, batch_y\n",
    "                batch_x = None\n",
    "                batch_y = None\n",
    "                batch_i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e35540c-7ae8-4f38-ae70-a2d38bc1e189",
    "_uuid": "07d54b2346e24ebe9fd6dad691e129bd721a8b45"
   },
   "source": [
    "Alright, now we can use this generator to train the network. To make sure that one epoch has approxamitely the same number of examples as are in the training set, we need to set the `steps_per_epoch` to the number of batches we expect to cover the whole dataset. Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "48f4dcad-b7fe-4cd6-81a7-9ac41e9a0daa",
    "_uuid": "1b55016b60964208c999381f09600af1a5ae96ed"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_cell_guid": "53202998-9f82-48eb-a8a7-86c0dcdd92fa",
    "_uuid": "876ed29154a6bdf94048af7dc0e6f5115b4f9f2e"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs = Input(shape=(150,))\n",
    "    inp = Reshape((1,150,))(inputs)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(inp)\n",
    "    #att = Attention(150)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    output = Dropout(0.5)(conc)\n",
    "    output = BatchNormalization()(output)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(output)\n",
    "    nadam = Nadam(lr=0.001)\n",
    "    model = Model(inputs=inputs, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=nadam,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_folds(X, y, fold_count, model_list, model_name):\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    total_meta = []\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "            \n",
    "        if fold_id == fold_count - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = X[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "          \n",
    "        save_path = os.path.join('models', '%s_model.h5' % (model_name + str(fold_id)))\n",
    "        callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_path, save_best_only=True, verbose=True)\n",
    "        ]\n",
    "        #train_x = np.reshape(train_x, train_x.shape + (1,))\n",
    "        training_generator = data_generator(train_x, 128)\n",
    "        x_tra = len(train_x)\n",
    "        training_steps_per_epoch = round(len(train_x) / batch_size)\n",
    "        model = train_model(model_list[fold_id], training_generator, x_tra, train_y, val_x, val_y,callbacks, training_steps_per_epoch)\n",
    "        meta = model.predict(val_x, batch_size=128)\n",
    "        if (fold_id == 0):\n",
    "            total_meta = meta\n",
    "        else:\n",
    "            total_meta = np.concatenate((total_meta, meta), axis=0)\n",
    "        model_path = os.path.join('models', \"model{0}_weights.npy\".format(fold_id))\n",
    "        np.save(model_path, model.get_weights())\n",
    "        models.append(model)\n",
    "\n",
    "    return models, total_meta\n",
    "\n",
    "def train_model(model, training_generator,x_tra, train_y, val_x, val_y, callbacks, training_steps_per_epoch):\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    current_epoch = 0\n",
    "    #charCNN:LSTM\n",
    "    #train_x = np.reshape(train_x, train_x.shape + (1,))\n",
    "    #val_x = np.reshape(val_x, val_x.shape + (1,))\n",
    "    exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "    steps = int(x_tra/batch_size) * 1000\n",
    "    lr_init, lr_fin = 0.001, 0.0005\n",
    "    lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "    #K.set_value(model.optimizer.lr, lr_init)\n",
    "    #K.set_value(model.optimizer.decay, lr_decay)\n",
    "\n",
    "    while True:\n",
    "        model.fit_generator(\n",
    "            training_generator,\n",
    "            steps_per_epoch=training_steps_per_epoch,\n",
    "            epochs=1,\n",
    "            validation_data=(val_x, val_y),\n",
    "            callbacks=callbacks,\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(val_x, batch_size=128)\n",
    "\n",
    "        total_loss = 0\n",
    "        for j in range(6):\n",
    "            loss = log_loss(val_y[:, j], y_pred[:, j])\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= 6.\n",
    "\n",
    "        print(\"Epoch {0} auc {1} best_auc {2}\".format(current_epoch, total_loss, best_loss))\n",
    "        \n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss < best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == 5:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_cell_guid": "1be4d47d-6b43-4422-b77f-f103e784e721",
    "_kg_hide-output": true,
    "_uuid": "f5b8b97f068e2884bc5e877a19afbc721ba135da"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_175 to have shape (150,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-d13403b08a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bigru\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtraining_steps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_models\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model trained!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-1831c82292a9>\u001b[0m in \u001b[0;36mtrain_folds\u001b[0;34m(X, y, fold_count, model_list, model_name)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx_tra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtraining_steps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_steps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfold_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-1831c82292a9>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, training_generator, x_tra, train_y, val_x, val_y, callbacks, training_steps_per_epoch)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2181\u001b[0m                                          str(validation_data))\n\u001b[1;32m   2182\u001b[0m                     val_x, val_y, val_sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 2183\u001b[0;31m                         val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m   2184\u001b[0m                     \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_175 to have shape (150,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "list_models = []\n",
    "folds = 10\n",
    "for fold in range(0, folds):\n",
    "    model = build_model()\n",
    "    list_models.append(model)\n",
    "train_labels = train[['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "batch_size = 128\n",
    "model_name = \"bigru\"\n",
    "training_steps_per_epoch = round(len(train['comment_text']) / batch_size)\n",
    "models, total_meta = train_folds(train['comment_text'], train_labels, folds, list_models,model_name)    \n",
    "print('Model trained!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting results...\")\n",
    "random_test = pd.read_csv('cleaned_test_clean.csv')\n",
    "#random_test = self.Sanitize(random_test)\n",
    "#random_test.to_csv('cleaned_test_clean.csv', index=False)\n",
    "X_test = random_test['comment_text'].fillna('_empty_')\n",
    "        X_test = self.prep_text(X_test)\n",
    "        #X_test = self.load_data(X_test)\n",
    "        test_predicts_list = []\n",
    "        for fold_id, model in enumerate(models):\n",
    "            model_path = os.path.join(self.model_dir, \"model{0}_weights.npy\".format(fold_id))\n",
    "            np.save(model_path, model.get_weights())\n",
    "        \n",
    "            test_predicts_path = os.path.join(self.model_dir, \"test_predicts{0}.npy\".format(fold_id))\n",
    "            test_predicts = model.predict(X_test, batch_size=self.hparams['batch_size'])\n",
    "            test_predicts_list.append(test_predicts)\n",
    "            np.save(test_predicts_path, test_predicts)\n",
    "\n",
    "        test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "        for fold_predict in test_predicts_list:\n",
    "            test_predicts *= fold_predict\n",
    "\n",
    "        test_predicts **= (1. / len(test_predicts_list))\n",
    "        test_ids = random_test[\"id\"].values\n",
    "        test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "        CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "        test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "        test_predicts[\"id\"] = test_ids\n",
    "        test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "        test_predicts.to_csv('augmentori_pred_fasttext_gru_cv_output_two.csv', index=False)\n",
    "        print('predicted !')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1af33152-8655-4aba-976e-b34a573940e8",
    "_uuid": "e2b848800743d084611e07d9f93fefe444eb2f88"
   },
   "source": [
    "And there you go, this should work on p2.xlarge even for long window lengths!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9697a31-c4aa-4ec7-85f8-f93f95b57228",
    "_uuid": "24d94ddbe3041de9a0ae252adb22b2d1354b3cff"
   },
   "source": [
    "### More stuff to try:\n",
    "Some suggestions. I've tried most of these and found them helpful:\n",
    "\n",
    "* Add random but common typos to strings before converting to FT vectors. That way, the model can learn in which way typos affect the embeddings. Use the training generator so you can adjust this over time.\n",
    "* Add more string preprocessing to our `normalize` function\n",
    "* Randomize the windows instead of using the end (great that we already have a generator!)\n",
    "* Use FastText's sentence vector feature to summarize parts of the text outside the window\n",
    "* Add other features ontop of the FT ones, e.g. capitalization etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
