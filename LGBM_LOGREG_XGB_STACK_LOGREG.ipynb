{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_predictions(prediction_dir, concat_mode='concat', per_label=False):\n",
    "    labels = pd.read_csv(os.path.join(prediction_dir, 'labels.csv'))\n",
    "\n",
    "    filepaths_train, filepaths_test = [], []\n",
    "    for filepath in sorted(glob.glob('{}/*'.format(prediction_dir))):\n",
    "        if filepath.endswith('predictions_test_oof.csv'):\n",
    "            filepaths_test.append(filepath)\n",
    "\n",
    "    test_dfs = []\n",
    "    for filepath in filepaths_test:\n",
    "        test_dfs.append(pd.read_csv(filepath))\n",
    "    test_dfs = reduce(lambda df1, df2: pd.merge(df1, df2, on=['id', 'fold_id']), test_dfs)\n",
    "    test_dfs.columns = _clean_columns(test_dfs, keep_colnames = ['id','fold_id'])\n",
    "\n",
    "    return train_dfs, test_dfs\n",
    "\n",
    "def get_fold_xy(test,i):\n",
    "    #train_split = train[train['fold_id'] != i]\n",
    "    #valid_split = train[train['fold_id'] == i]\n",
    "    test_split = test[test['fold_id'] == i]\n",
    "\n",
    "    #y_train = train_split[label_columns].values\n",
    "    #y_valid = valid_split[label_columns].values\n",
    "    #columns_to_drop_train = label_columns + ['id','fold_id']\n",
    "    #X_train = train_split.drop(columns_to_drop_train, axis=1).values\n",
    "    #X_valid = valid_split.drop(columns_to_drop_train, axis=1).values\n",
    "\n",
    "    columns_to_drop_test = ['id','fold_id']\n",
    "    X_test = test_split.drop(columns_to_drop_test, axis=1).values\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File newstageone/13/word2vec_scnn_predictions_test_oof.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0e23cb828c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpr_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'newstageone/13/word2vec_scnn_predictions_test_oof.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_predicts_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dcek/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dcek/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dcek/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dcek/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dcek/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File newstageone/13/word2vec_scnn_predictions_test_oof.csv does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pr_file = pd.read_csv('newstageone/13/word2vec_scnn_predictions_test_oof.csv')\n",
    "test_predicts_list = []\n",
    "for fold in range(0,10):\n",
    "    get_fold_xy(pr_file,fold)\n",
    "    test_predicts_list.append(get_fold_xy(pr_file,fold))\n",
    "#pr_file.to_csv(\"newstageone/13/bad_word_logreg_predictions_test_oof.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('cleaned_test.csv')\n",
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "test_ids = test[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "test_predicts.to_csv('newstageone/13/word2vec_scnn_predictions_test_oof.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('newstageone/OOF/oof20.csv')\n",
    "test.head()\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning, module='sklearn')\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#######################\n",
    "# FEATURE ENGINEERING #\n",
    "#######################\n",
    "\"\"\"\n",
    "Main function\n",
    "Input: pandas Series and a feature engineering function\n",
    "Output: pandas Series\n",
    "\"\"\"\n",
    "def engineer_feature(series, func, normalize=True):\n",
    "    feature = series.apply(func)\n",
    "       \n",
    "    if normalize:\n",
    "        feature = pd.Series(z_normalize(feature.values.reshape(-1,1)).reshape(-1,))\n",
    "    feature.name = func.__name__ \n",
    "    return feature\n",
    "\n",
    "\"\"\"\n",
    "Engineer features\n",
    "Input: pandas Series and a list of feature engineering functions\n",
    "Output: pandas DataFrame\n",
    "\"\"\"\n",
    "def engineer_features(series, funclist, normalize=True):\n",
    "    features = pd.DataFrame()\n",
    "    for func in funclist:\n",
    "        feature = engineer_feature(series, func, normalize)\n",
    "        features[feature.name] = feature\n",
    "    return features\n",
    "\n",
    "\"\"\"\n",
    "Normalizer\n",
    "Input: NumPy array\n",
    "Output: NumPy array\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "def z_normalize(data):\n",
    "    scaler.fit(data)\n",
    "    return scaler.transform(data)\n",
    "    \n",
    "\"\"\"\n",
    "Feature functions\n",
    "\"\"\"\n",
    "def asterix_freq(x):\n",
    "    return x.count('!')/len(x)\n",
    "\n",
    "def uppercase_freq(x):\n",
    "    return len(re.findall(r'[A-Z]',x))/len(x)\n",
    "    \n",
    "\"\"\"\n",
    "Import submission and OOF files\n",
    "\"\"\"\n",
    "def get_subs(nums):\n",
    "    subs = np.hstack([np.array(pd.read_csv(\"SUB/sub\" + str(num) + \".csv\")[LABELS]) for num in subnums])\n",
    "    oofs = np.hstack([np.array(pd.read_csv(\"OOF/oof\" + str(num) + \".csv\")[LABELS]) for num in subnums])\n",
    "    return subs, oofs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_folds(X, y, fold_count, model_list):\n",
    "      fold_size = len(X) // fold_count\n",
    "      models = []\n",
    "      total_meta = []\n",
    "      auc_list = []\n",
    "      for fold_id in range(0, fold_count):\n",
    "          print(\"FOLD {}\".format(fold_id))\n",
    "          fold_start = fold_size * fold_id\n",
    "          fold_end = fold_start + fold_size\n",
    "            \n",
    "          if fold_id == fold_count - 1:\n",
    "              fold_end = len(X)\n",
    "\n",
    "          train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "          train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "          val_x = X[fold_start:fold_end]\n",
    "          val_y = y[fold_start:fold_end]\n",
    "            \n",
    "          \n",
    "          model, best_auc = _train_model(model_list[fold_id], train_x, train_y, val_x, val_y,callbacks)\n",
    "            \n",
    "          meta = model.predict(val_x, batch_size=128)\n",
    "          if (fold_id == 0):\n",
    "              total_meta = meta\n",
    "          else:\n",
    "              total_meta = np.concatenate((total_meta, meta), axis=0)\n",
    "          model_path = os.path.join('models', \"model{0}_weights.npy\".format(fold_id))\n",
    "          np.save(model_path, model.get_weights())\n",
    "          models.append(model)\n",
    "          auc_list.append(best_auc)\n",
    "\n",
    "      return models, total_meta, auc_list\n",
    "\n",
    "def _train_model(model, train_x, train_y, val_x, val_y):\n",
    "    for label in LABELS:\n",
    "        model = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "        model.fit(\n",
    "                train_x,\n",
    "                train_x[label])\n",
    "        \n",
    "        y_pred = model.predict_proba(val_x)[:,1]\n",
    "\n",
    "    total_auc = 0\n",
    "    for j in range(6):\n",
    "        auc = compute_auc(val_y[:, j], y_pred[:, j])\n",
    "        total_auc += auc\n",
    "\n",
    "    total_loss /= 6.\n",
    "    total_auc /= 6.\n",
    "    return model, total_auc\n",
    "    \n",
    "def build_model():\n",
    "    stackers = []\n",
    "    for fold in range(0,10):\n",
    "        stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "        stackers.append(stacker)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Scoring Light GBM\"):\n",
    "    scores = []\n",
    "    folds = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "    lgb_round_dict = defaultdict(int)\n",
    "    trn_lgbset = lgb.Dataset(csr_trn, free_raw_data=False)\n",
    "    del csr_trn\n",
    "    gc.collect()\n",
    "        \n",
    "    for class_name in class_names:\n",
    "        print(\"Class %s scores : \" % class_name)\n",
    "        class_pred = np.zeros(len(train))\n",
    "        train_target = train[class_name]\n",
    "        trn_lgbset.set_label(train_target.values)\n",
    "            \n",
    "        lgb_rounds = 500\n",
    "\n",
    "        for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, train_target)):\n",
    "            watchlist = [\n",
    "                trn_lgbset.subset(trn_idx),\n",
    "                trn_lgbset.subset(val_idx)\n",
    "            ]\n",
    "            # Train lgb l1\n",
    "            model = lgb.train(\n",
    "                params=params,\n",
    "                train_set=watchlist[0],\n",
    "                num_boost_round=lgb_rounds,\n",
    "                valid_sets=watchlist,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=0\n",
    "            )\n",
    "            class_pred[val_idx] = model.predict(trn_lgbset.data[val_idx], num_iteration=model.best_iteration)\n",
    "            score = roc_auc_score(train_target.values[val_idx], class_pred[val_idx])\n",
    "                \n",
    "            # Compute mean rounds over folds for each class\n",
    "            # So that it can be re-used for test predictions\n",
    "            lgb_round_dict[class_name] += model.best_iteration\n",
    "            print(\"\\t Fold %d : %.6f in %3d rounds\" % (n_fold + 1, score, model.best_iteration))\n",
    "            \n",
    "        print(\"full score : %.6f\" % roc_auc_score(train_target, class_pred))\n",
    "        scores.append(roc_auc_score(train_target, class_pred))\n",
    "        train[class_name + \"_oof\"] = class_pred\n",
    "        submission[class_name] = lr_pred / folds\n",
    "\n",
    "    # Save OOF predictions - may be interesting for stacking...\n",
    "    train[[\"id\"] + class_names + [f + \"_oof\" for f in class_names]].to_csv(\"lvl0_lgbm_clean_oof.csv\",\n",
    "                                                                            index=False,\n",
    "                                                                            float_format=\"%.8f\")\n",
    "\n",
    "    print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "with timer(\"Predicting probabilities\"):\n",
    "    # Go through all classes and reuse computed number of rounds for each class\n",
    "    for class_name in class_names:\n",
    "        with timer(\"Predicting probabilities for %s\" % class_name):\n",
    "            train_target = train[class_name]\n",
    "            trn_lgbset.set_label(train_target.values)\n",
    "            # Train lgb\n",
    "            model = lgb.train(\n",
    "                params=params,\n",
    "                train_set=trn_lgbset,\n",
    "                num_boost_round=int(lgb_round_dict[class_name] / folds.n_splits)\n",
    "            )\n",
    "            submission[class_name] = model.predict(csr_sub, num_iteration=model.best_iteration)\n",
    "\n",
    "submission.to_csv(\"lvl0_lgbm_clean_sub.csv\", index=False, float_format=\"%.8f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average class toxic AUC:\t0.987802\n",
      " Out-of-fold class toxic AUC:\t0.987650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stgc/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average class severe_toxic AUC:\t0.991559\n",
      " Out-of-fold class severe_toxic AUC:\t0.991465\n",
      "\n",
      " Average class obscene AUC:\t0.995286\n",
      " Out-of-fold class obscene AUC:\t0.995268\n",
      "\n",
      " Average class threat AUC:\t0.991848\n",
      " Out-of-fold class threat AUC:\t0.991016\n",
      "\n",
      " Average class insult AUC:\t0.989942\n",
      " Out-of-fold class insult AUC:\t0.989909\n",
      "\n",
      " Average class identity_hate AUC:\t0.990874\n",
      " Out-of-fold class identity_hate AUC:\t0.990406\n",
      "\n",
      " Overall AUC:\t0.991219\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train = pd.read_csv('train.csv').fillna(' ')\n",
    "    test = pd.read_csv('test.csv').fillna(' ')\n",
    "    sub = pd.read_csv('sample_submission.csv')\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    submission_oof = train[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "    INPUT_COLUMN = \"comment_text\"\n",
    "    LABELS = train.columns[2:]\n",
    "    \n",
    "    # Import submissions and OOF files\n",
    "    # 29: LightGBM trained on Fasttext (CV: 0.9765, LB: 0.9620)\n",
    "    # 51: Logistic regression with word and char n-grams (CV: 0.9858, LB: ?)\n",
    "    # 52: LSTM trained on Fasttext (CV: ?, LB: 0.9851)\n",
    "    subnums = [1,2,3,4,5,6,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]\n",
    "    subs, oofs = get_subs(subnums)\n",
    "    \n",
    "    # Engineer features\n",
    "    feature_functions = [len, asterix_freq, uppercase_freq]\n",
    "    features = [f.__name__ for f in feature_functions]\n",
    "    F_train = engineer_features(train[INPUT_COLUMN], feature_functions)\n",
    "    F_test = engineer_features(test[INPUT_COLUMN], feature_functions)\n",
    "    \n",
    "    train_features = np.hstack([F_train[features].as_matrix(), oofs])\n",
    "    X_test = np.hstack([F_test[features].as_matrix(), subs]) \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    \n",
    "    scores_classes = np.zeros((len(LABELS), 10))\n",
    "    for j, (class_name) in enumerate(LABELS):\n",
    "        avreal = train[class_name]\n",
    "        lr_avpred = np.zeros(train.shape[0])\n",
    "        #stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_features, train[class_name].values)):\n",
    "            #print(train_index)\n",
    "            stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "            X_train, X_val = train_features[train_index], train_features[val_index]\n",
    "            y_train, y_val = train.loc[train_index], train.loc[val_index]\n",
    "            stacker.fit(X_train, y_train[class_name])\n",
    "            \n",
    "            scores_val = stacker.predict_proba(X_val)[:, 1]\n",
    "            scores_classes[j][i] = roc_auc_score(y_val[class_name], scores_val)\n",
    "            lr_avpred[val_index] = scores_val\n",
    "            \n",
    "            lr_y_pred = stacker.predict_proba(X_test)[:, 1]\n",
    "            if i > 0:\n",
    "                lr_fpred = lr_pred + lr_y_pred\n",
    "            else:\n",
    "                lr_fpred = lr_y_pred\n",
    "\n",
    "            lr_pred = lr_fpred\n",
    "            \n",
    "        lr_oof_auc = roc_auc_score(avreal, lr_avpred)\n",
    "        print('\\n Average class %s AUC:\\t%.6f' % (class_name, np.mean(scores_classes[j])))\n",
    "        print(' Out-of-fold class %s AUC:\\t%.6f' % (class_name, lr_oof_auc))\n",
    "        submission[class_name] = lr_pred / 10\n",
    "        submission_oof[class_name] = lr_avpred\n",
    "    \n",
    "    print('\\n Overall AUC:\\t%.6f' % (np.mean(scores_classes)))\n",
    "    submission.to_csv('lgb_stacktwo_pred.csv', index=False)\n",
    "    submission_oof.to_csv('lgb_stacktwo_meta.csv', index=False)\n",
    "\n",
    "    '''\n",
    "    stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "    \n",
    "    # Fit and submit\n",
    "    \n",
    "    scores = []\n",
    "    for label in LABELS:\n",
    "        print(label)\n",
    "        score = cross_val_score(stacker, X_train, train[label], cv=10, scoring='roc_auc')\n",
    "        print(\"AUC:\", score)\n",
    "        scores.append(np.mean(score))\n",
    "        stacker.fit(X_train, train[label])\n",
    "        sub[label] = stacker.predict_proba(X_test)[:,1]\n",
    "    print(\"CV score:\", np.mean(scores))\n",
    "    \n",
    "    sub.to_csv(\"29modelstack.csv\", index=False)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average class toxic AUC:\t0.987147\n",
      " Out-of-fold class toxic AUC:\t0.987087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stgc/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average class severe_toxic AUC:\t0.991326\n",
      " Out-of-fold class severe_toxic AUC:\t0.991270\n",
      "\n",
      " Average class obscene AUC:\t0.994503\n",
      " Out-of-fold class obscene AUC:\t0.994479\n",
      "\n",
      " Average class threat AUC:\t0.988594\n",
      " Out-of-fold class threat AUC:\t0.988375\n",
      "\n",
      " Average class insult AUC:\t0.988594\n",
      " Out-of-fold class insult AUC:\t0.988550\n",
      "\n",
      " Average class identity_hate AUC:\t0.988470\n",
      " Out-of-fold class identity_hate AUC:\t0.988419\n",
      "\n",
      " Overall AUC:\t0.989772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train = pd.read_csv('train.csv').fillna(' ')\n",
    "    test = pd.read_csv('test.csv').fillna(' ')\n",
    "    sub = pd.read_csv('sample_submission.csv')\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    submission_oof = train[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "    INPUT_COLUMN = \"comment_text\"\n",
    "    LABELS = train.columns[2:]\n",
    "    \n",
    "    # Import submissions and OOF files\n",
    "    # 29: LightGBM trained on Fasttext (CV: 0.9765, LB: 0.9620)\n",
    "    # 51: Logistic regression with word and char n-grams (CV: 0.9858, LB: ?)\n",
    "    # 52: LSTM trained on Fasttext (CV: ?, LB: 0.9851)\n",
    "    subnums = [1,2,3,4,5,6,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]\n",
    "    subs, oofs = get_subs(subnums)\n",
    "    \n",
    "    # Engineer features\n",
    "    feature_functions = [len, asterix_freq, uppercase_freq]\n",
    "    features = [f.__name__ for f in feature_functions]\n",
    "    F_train = engineer_features(train[INPUT_COLUMN], feature_functions)\n",
    "    F_test = engineer_features(test[INPUT_COLUMN], feature_functions)\n",
    "    \n",
    "    train_features = np.hstack([F_train[features].as_matrix(), oofs])\n",
    "    X_test = np.hstack([F_test[features].as_matrix(), subs]) \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    \n",
    "    scores_classes = np.zeros((len(LABELS), 10))\n",
    "    all_parameters = {\n",
    "                  'C'             : [1.048113, 0.1930, 0.596362, 0.25595, 0.449843, 0.25595],\n",
    "                  'tol'           : [0.1, 0.1, 0.046416, 0.0215443, 0.1, 0.01],\n",
    "                  'solver'        : ['lbfgs', 'newton-cg', 'lbfgs', 'newton-cg', 'newton-cg', 'lbfgs'],\n",
    "                  'fit_intercept' : [True, True, True, True, True, True],\n",
    "                  'penalty'       : ['l2', 'l2', 'l2', 'l2', 'l2', 'l2'],\n",
    "                  'class_weight'  : [None, 'balanced', 'balanced', 'balanced', 'balanced', 'balanced'],\n",
    "                 }\n",
    "    for j, (class_name) in enumerate(LABELS):\n",
    "        avreal = train[class_name]\n",
    "        lr_avpred = np.zeros(train.shape[0])\n",
    "        #stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_features, train[class_name].values)):\n",
    "            #print(train_index)\n",
    "            stacker = LogisticRegression(\n",
    "                C=all_parameters['C'][j],\n",
    "                max_iter=200,\n",
    "                tol=all_parameters['tol'][j],\n",
    "                solver=all_parameters['solver'][j],\n",
    "                fit_intercept=all_parameters['fit_intercept'][j],\n",
    "                penalty=all_parameters['penalty'][j],\n",
    "                dual=False,\n",
    "                class_weight=all_parameters['class_weight'][j],\n",
    "                verbose=0)\n",
    "            X_train, X_val = train_features[train_index], train_features[val_index]\n",
    "            y_train, y_val = train.loc[train_index], train.loc[val_index]\n",
    "            stacker.fit(X_train, y_train[class_name])\n",
    "            \n",
    "            scores_val = stacker.predict_proba(X_val)[:, 1]\n",
    "            scores_classes[j][i] = roc_auc_score(y_val[class_name], scores_val)\n",
    "            lr_avpred[val_index] = scores_val\n",
    "            \n",
    "            lr_y_pred = stacker.predict_proba(X_test)[:, 1]\n",
    "            if i > 0:\n",
    "                lr_fpred = lr_pred + lr_y_pred\n",
    "            else:\n",
    "                lr_fpred = lr_y_pred\n",
    "\n",
    "            lr_pred = lr_fpred\n",
    "            \n",
    "        lr_oof_auc = roc_auc_score(avreal, lr_avpred)\n",
    "        print('\\n Average class %s AUC:\\t%.6f' % (class_name, np.mean(scores_classes[j])))\n",
    "        print(' Out-of-fold class %s AUC:\\t%.6f' % (class_name, lr_oof_auc))\n",
    "        submission[class_name] = lr_pred / 10\n",
    "        submission_oof[class_name] = lr_avpred\n",
    "    \n",
    "    print('\\n Overall AUC:\\t%.6f' % (np.mean(scores_classes)))\n",
    "    submission.to_csv('logisticreg_stacktwo_pred.csv', index=False)\n",
    "    submission_oof.to_csv('logisticreg_stacktwo_meta.csv', index=False)\n",
    "\n",
    "    '''\n",
    "    stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "    \n",
    "    # Fit and submit\n",
    "    \n",
    "    scores = []\n",
    "    for label in LABELS:\n",
    "        print(label)\n",
    "        score = cross_val_score(stacker, X_train, train[label], cv=10, scoring='roc_auc')\n",
    "        print(\"AUC:\", score)\n",
    "        scores.append(np.mean(score))\n",
    "        stacker.fit(X_train, train[label])\n",
    "        sub[label] = stacker.predict_proba(X_test)[:,1]\n",
    "    print(\"CV score:\", np.mean(scores))\n",
    "    \n",
    "    sub.to_csv(\"29modelstack.csv\", index=False)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average class toxic AUC:\t0.987652\n",
      " Out-of-fold class toxic AUC:\t0.987478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stgc/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average class severe_toxic AUC:\t0.991304\n",
      " Out-of-fold class severe_toxic AUC:\t0.991147\n",
      "\n",
      " Average class obscene AUC:\t0.995268\n",
      " Out-of-fold class obscene AUC:\t0.995247\n",
      "\n",
      " Average class threat AUC:\t0.989995\n",
      " Out-of-fold class threat AUC:\t0.989601\n",
      "\n",
      " Average class insult AUC:\t0.989765\n",
      " Out-of-fold class insult AUC:\t0.989735\n",
      "\n",
      " Average class identity_hate AUC:\t0.990650\n",
      " Out-of-fold class identity_hate AUC:\t0.990048\n",
      "\n",
      " Overall AUC:\t0.990772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train = pd.read_csv('train.csv').fillna(' ')\n",
    "    test = pd.read_csv('test.csv').fillna(' ')\n",
    "    sub = pd.read_csv('sample_submission.csv')\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    submission_oof = train[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "    INPUT_COLUMN = \"comment_text\"\n",
    "    LABELS = train.columns[2:]\n",
    "    \n",
    "    # Import submissions and OOF files\n",
    "    # 29: LightGBM trained on Fasttext (CV: 0.9765, LB: 0.9620)\n",
    "    # 51: Logistic regression with word and char n-grams (CV: 0.9858, LB: ?)\n",
    "    # 52: LSTM trained on Fasttext (CV: ?, LB: 0.9851)\n",
    "    subnums = [1,2,3,4,5,6,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]\n",
    "    subs, oofs = get_subs(subnums)\n",
    "    \n",
    "    # Engineer features\n",
    "    feature_functions = [len, asterix_freq, uppercase_freq]\n",
    "    features = [f.__name__ for f in feature_functions]\n",
    "    F_train = engineer_features(train[INPUT_COLUMN], feature_functions)\n",
    "    F_test = engineer_features(test[INPUT_COLUMN], feature_functions)\n",
    "    \n",
    "    train_features = np.hstack([F_train[features].as_matrix(), oofs])\n",
    "    X_test = np.hstack([F_test[features].as_matrix(), subs]) \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    \n",
    "    scores_classes = np.zeros((len(LABELS), 10))\n",
    "\n",
    "    for j, (class_name) in enumerate(LABELS):\n",
    "        avreal = train[class_name]\n",
    "        lr_avpred = np.zeros(train.shape[0])\n",
    "        #stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_features, train[class_name].values)):\n",
    "            #print(train_index)\n",
    "            n_estimators = 200\n",
    "            stacker = clf = XGBClassifier(n_estimators=n_estimators,\n",
    "                        max_depth=4,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=.1, \n",
    "                        subsample=.8, \n",
    "                        colsample_bytree=.8,\n",
    "                        gamma=1,\n",
    "                        reg_alpha=0,\n",
    "                        reg_lambda=1,\n",
    "                        nthread=2)\n",
    "            X_train, X_val = train_features[train_index], train_features[val_index]\n",
    "            y_train, y_val = train.loc[train_index], train.loc[val_index]\n",
    "            stacker.fit(X_train, y_train[class_name])\n",
    "            \n",
    "            scores_val = stacker.predict_proba(X_val)[:, 1]\n",
    "            scores_classes[j][i] = roc_auc_score(y_val[class_name], scores_val)\n",
    "            lr_avpred[val_index] = scores_val\n",
    "            \n",
    "            lr_y_pred = stacker.predict_proba(X_test)[:, 1]\n",
    "            if i > 0:\n",
    "                lr_fpred = lr_pred + lr_y_pred\n",
    "            else:\n",
    "                lr_fpred = lr_y_pred\n",
    "\n",
    "            lr_pred = lr_fpred\n",
    "            \n",
    "        lr_oof_auc = roc_auc_score(avreal, lr_avpred)\n",
    "        print('\\n Average class %s AUC:\\t%.6f' % (class_name, np.mean(scores_classes[j])))\n",
    "        print(' Out-of-fold class %s AUC:\\t%.6f' % (class_name, lr_oof_auc))\n",
    "        submission[class_name] = lr_pred / 10\n",
    "        submission_oof[class_name] = lr_avpred\n",
    "    \n",
    "    print('\\n Overall AUC:\\t%.6f' % (np.mean(scores_classes)))\n",
    "    submission.to_csv('XGB_stacktwo_pred.csv', index=False)\n",
    "    submission_oof.to_csv('XGB_stacktwo_meta.csv', index=False)\n",
    "\n",
    "    '''\n",
    "    stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "    \n",
    "    # Fit and submit\n",
    "    \n",
    "    scores = []\n",
    "    for label in LABELS:\n",
    "        print(label)\n",
    "        score = cross_val_score(stacker, X_train, train[label], cv=10, scoring='roc_auc')\n",
    "        print(\"AUC:\", score)\n",
    "        scores.append(np.mean(score))\n",
    "        stacker.fit(X_train, train[label])\n",
    "        sub[label] = stacker.predict_proba(X_test)[:,1]\n",
    "    print(\"CV score:\", np.mean(scores))\n",
    "    \n",
    "    sub.to_csv(\"29modelstack.csv\", index=False)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden_features(data):\n",
    "    df = pd.DataFrame()\n",
    "    df['total_length'] = data['comment_text'].apply(len)\n",
    "    df['capitals'] = data['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_exclamation_marks'] = data['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = data['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = data['comment_text'].apply(\n",
    "        lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['num_symbols'] = data['comment_text'].apply(\n",
    "        lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "    df['num_words'] = data['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "    df['num_unique_words'] = data['comment_text'].apply(\n",
    "        lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    df['num_smilies'] = data['comment_text'].apply(\n",
    "        lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average class toxic AUC:\t0.988588\n",
      " Out-of-fold class toxic AUC:\t0.988451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stgc/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average class severe_toxic AUC:\t0.991879\n",
      " Out-of-fold class severe_toxic AUC:\t0.991808\n",
      "\n",
      " Average class obscene AUC:\t0.995325\n",
      " Out-of-fold class obscene AUC:\t0.995282\n",
      "\n",
      " Average class threat AUC:\t0.990770\n",
      " Out-of-fold class threat AUC:\t0.990069\n",
      "\n",
      " Average class insult AUC:\t0.989822\n",
      " Out-of-fold class insult AUC:\t0.989714\n",
      "\n",
      " Average class identity_hate AUC:\t0.990839\n",
      " Out-of-fold class identity_hate AUC:\t0.990254\n",
      "\n",
      " Overall AUC:\t0.991204\n",
      "toxic\n",
      "('AUC:', array([0.98889795, 0.98861989, 0.98971198, 0.9868848 , 0.98862791,\n",
      "       0.9890668 , 0.98846785, 0.98917377, 0.98753618, 0.9888971 ]))\n",
      "severe_toxic\n",
      "('AUC:', array([0.99273583, 0.99111359, 0.99293898, 0.99054152, 0.99058346,\n",
      "       0.99194787, 0.99232698, 0.99152614, 0.99322736, 0.99184623]))\n",
      "obscene\n",
      "('AUC:', array([0.99580642, 0.99464038, 0.99561256, 0.99475678, 0.9959103 ,\n",
      "       0.99596112, 0.99524442, 0.99531185, 0.99552059, 0.99448298]))\n",
      "threat\n",
      "('AUC:', array([0.98517704, 0.99588571, 0.99611617, 0.98161418, 0.98804791,\n",
      "       0.98691385, 0.9933096 , 0.99055304, 0.99403656, 0.99604131]))\n",
      "insult\n",
      "('AUC:', array([0.98973901, 0.99019773, 0.98993672, 0.98867237, 0.98993016,\n",
      "       0.99099737, 0.9898002 , 0.98913562, 0.99063964, 0.9891739 ]))\n",
      "identity_hate\n",
      "('AUC:', array([0.98968835, 0.99280467, 0.98910903, 0.99082099, 0.99160926,\n",
      "       0.99028667, 0.98763187, 0.98932094, 0.99433892, 0.99277969]))\n",
      "('CV score:', 0.9912038016791836)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_subs(nums):\n",
    "    subs = np.hstack([np.array(pd.read_csv(\"SUB_two/sub\" + str(num) + \".csv\")[LABELS]) for num in subnums])\n",
    "    oofs = np.hstack([np.array(pd.read_csv(\"OOF_two/oof\" + str(num) + \".csv\")[LABELS]) for num in subnums])\n",
    "    return subs, oofs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train = pd.read_csv('train.csv').fillna(' ')\n",
    "    test = pd.read_csv('test.csv').fillna(' ')\n",
    "    sub = pd.read_csv('sample_submission.csv')\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    submission_oof = train[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "    INPUT_COLUMN = \"comment_text\"\n",
    "    LABELS = train.columns[2:]\n",
    "    \n",
    "    # Import submissions and OOF files\n",
    "    # 29: LightGBM trained on Fasttext (CV: 0.9765, LB: 0.9620)\n",
    "    # 51: Logistic regression with word and char n-grams (CV: 0.9858, LB: ?)\n",
    "    # 52: LSTM trained on Fasttext (CV: ?, LB: 0.9851)\n",
    "    subnums = [1,2,3]\n",
    "    subs, oofs = get_subs(subnums)\n",
    "    \n",
    "    # Engineer features\n",
    "    feature_functions = [len, asterix_freq, uppercase_freq]\n",
    "    features = [f.__name__ for f in feature_functions]\n",
    "    F_train = engineer_features(train[INPUT_COLUMN], feature_functions)\n",
    "    F_test = engineer_features(test[INPUT_COLUMN], feature_functions)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_features_pri = np.hstack([F_train[features].as_matrix(), oofs])\n",
    "    X_test_pri = np.hstack([F_test[features].as_matrix(), subs]) \n",
    "    \n",
    "    gold_F = ('total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks',\n",
    "            'num_question_marks', 'num_punctuation', 'num_words', 'num_unique_words',\n",
    "            'words_vs_unique', 'num_smilies', 'num_symbols')\n",
    "    gold_Feature = [g for g in gold_F]\n",
    "    \n",
    "    G_train = golden_features(train)\n",
    "    G_test = golden_features(test)\n",
    "    \n",
    "    train_features = np.hstack([G_train[gold_Feature].as_matrix(), train_features_pri])\n",
    "    X_test = np.hstack([G_test[gold_Feature].as_matrix(), X_test_pri])\n",
    "    \n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    \n",
    "    scores_classes = np.zeros((len(LABELS), 10))\n",
    "\n",
    "    for j, (class_name) in enumerate(LABELS):\n",
    "        avreal = train[class_name]\n",
    "        lr_avpred = np.zeros(train.shape[0])\n",
    "        #stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_features, train[class_name].values)):\n",
    "            #print(train_index)\n",
    "            stacker = stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "            X_train, X_val = train_features[train_index], train_features[val_index]\n",
    "            y_train, y_val = train.loc[train_index], train.loc[val_index]\n",
    "            stacker.fit(X_train, y_train[class_name])\n",
    "            \n",
    "            scores_val = stacker.predict_proba(X_val)[:, 1]\n",
    "            scores_classes[j][i] = roc_auc_score(y_val[class_name], scores_val)\n",
    "            lr_avpred[val_index] = scores_val\n",
    "            \n",
    "            lr_y_pred = stacker.predict_proba(X_test)[:, 1]\n",
    "            if i > 0:\n",
    "                lr_fpred = lr_pred + lr_y_pred\n",
    "            else:\n",
    "                lr_fpred = lr_y_pred\n",
    "\n",
    "            lr_pred = lr_fpred\n",
    "            \n",
    "        lr_oof_auc = roc_auc_score(avreal, lr_avpred)\n",
    "        print('\\n Average class %s AUC:\\t%.6f' % (class_name, np.mean(scores_classes[j])))\n",
    "        print(' Out-of-fold class %s AUC:\\t%.6f' % (class_name, lr_oof_auc))\n",
    "        submission[class_name] = lr_pred / 10\n",
    "        submission_oof[class_name] = lr_avpred\n",
    "    \n",
    "    print('\\n Overall AUC:\\t%.6f' % (np.mean(scores_classes)))\n",
    "    submission.to_csv('lgbm_stack_final_pred.csv', index=False)\n",
    "    submission_oof.to_csv('LGBM_stack_final_meta.csv', index=False)\n",
    "\n",
    "    \n",
    "    stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "    \n",
    "    # Fit and submit\n",
    "    \n",
    "    scores = []\n",
    "    for label in LABELS:\n",
    "        print(label)\n",
    "        score = cross_val_score(stacker, train_features, train[label], cv=10, scoring='roc_auc')\n",
    "        print(\"AUC:\", score)\n",
    "        scores.append(np.mean(score))\n",
    "        stacker.fit(train_features, train[label])\n",
    "        sub[label] = stacker.predict_proba(X_test)[:,1]\n",
    "    print(\"CV score:\", np.mean(scores))\n",
    "    \n",
    "    sub.to_csv(\"TRAIN_ALL_STACK_LOG_REG.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999646</td>\n",
       "      <td>0.992186</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.996217</td>\n",
       "      <td>0.997916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.016933</td>\n",
       "      <td>0.017819</td>\n",
       "      <td>0.020447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>0.016962</td>\n",
       "      <td>0.018510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>0.027373</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.017579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.005858</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.010354</td>\n",
       "      <td>0.025551</td>\n",
       "      <td>0.014992</td>\n",
       "      <td>0.019761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.999646      0.992186  0.999660  0.849823  0.996217   \n",
       "1  0000247867823ef7  0.006185      0.001481  0.008564  0.016933  0.017819   \n",
       "2  00013b17ad220c46  0.006356      0.001386  0.009035  0.017988  0.016962   \n",
       "3  00017563c3f7919a  0.004452      0.001463  0.008542  0.027373  0.013220   \n",
       "4  00017695ad8997eb  0.005858      0.001344  0.010354  0.025551  0.014992   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.997916  \n",
       "1       0.020447  \n",
       "2       0.018510  \n",
       "3       0.017579  \n",
       "4       0.019761  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('SUB_two/logisticreg_stacktwo_pred.csv')\n",
    "test.head()\n",
    "#columns_to_drop_test = ['toxic','severe_toxic','threat','insult','identity_hate','obscene']\n",
    "#columns_to_drop_test = ['comment_text']\n",
    "\n",
    "#test = test.drop(columns_to_drop_test, axis=1)\n",
    "\n",
    "#test = test.rename(columns={'toxic_oof': 'toxic', 'severe_toxic_oof': 'severe_toxic', 'obscene_oof': 'obscene', 'threat_oof': 'threat', 'insult_oof': 'insult', 'identity_hate_oof': 'identity_hate'})\n",
    "#test.to_csv(\"29modelstack.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File SUB_two/sub4.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-b5bac4d081b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moofs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-627530d0824c>\u001b[0m in \u001b[0;36mget_subs\u001b[0;34m(nums)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SUB_two/sub\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubnums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moofs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OOF_two/oof\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubnums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moofs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stgc/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stgc/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stgc/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stgc/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stgc/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File SUB_two/sub4.csv does not exist"
     ]
    }
   ],
   "source": [
    "subnums = [4]\n",
    "subs, oofs = get_subs(subnums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('OOF/oof28.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
